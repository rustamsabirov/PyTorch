{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8FsvOED8UlF"
   },
   "source": [
    "# Нейросети в обработке текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzjJy15I3r9U"
   },
   "source": [
    "## Домашнее задание\n",
    "1. Попробуйте обучить нейронную сеть с применением одномерных сверток для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
    "\n",
    "2. Опишите, какой результат вы получили? Что помогло вам улучшить ее точность?\n",
    "\n",
    "У кого нет возможности работать через каггл (нет верификации), то можете данные взять по ссылке: https://drive.google.com/file/d/1S0hslcWDrwxA5GH4U32mfg85lAVLqkq5/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context\n",
    "# The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate \n",
    "# speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from \n",
    "# other tweets.\n",
    "# Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' \n",
    "# denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.\n",
    "\n",
    "# Контекст\n",
    "# Цель этой задачи - обнаружить ненавистнические высказывания в твитах. Для простоты мы говорим, что твит содержит \n",
    "# ненавистнические высказывания, если с ним связаны расистские или сексистские настроения. Итак, задача состоит в том, чтобы \n",
    "# отличить расистские или сексистские твиты от других твитов.\n",
    "# Формально, учитывая обучающую выборку твитов и меток, где метка \"1\" означает, что твит является расистским/сексистским, \n",
    "# а метка \"0\" означает, что твит не является расистским /сексистским, ваша цель - предсказать метки в тестовом наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "pxrx8gGqpQe-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '\\PyTorch-6'\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\",index_col=0)\n",
    "df_test = pd.read_csv(\"test.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>0</td>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>0</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "id                                                             \n",
       "1          0   @user when a father is dysfunctional and is s...\n",
       "2          0  @user @user thanks for #lyft credit i can't us...\n",
       "3          0                                bihday your majesty\n",
       "4          0  #model   i love u take with u all the time in ...\n",
       "5          0             factsguide: society now    #motivation\n",
       "...      ...                                                ...\n",
       "31958      0  ate @user isz that youuu?ðððððð...\n",
       "31959      0    to see nina turner on the airwaves trying to...\n",
       "31960      0  listening to sad songs on a monday morning otw...\n",
       "31961      1  @user #sikh #temple vandalised in in #calgary,...\n",
       "31962      0                   thank you @user for you follow  \n",
       "\n",
       "[31962 rows x 2 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31962 entries, 1 to 31962\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   31962 non-null  int64 \n",
      " 1   tweet   31962 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 749.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31967</th>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>thought factory: left-right polarisation! #tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>feeling like a mermaid ð #hairflip #neverre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49159</th>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17197 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "id                                                      \n",
       "31963  #studiolife #aislife #requires #passion #dedic...\n",
       "31964   @user #white #supremacists want everyone to s...\n",
       "31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "31966  is the hp and the cursed child book up for res...\n",
       "31967    3rd #bihday to my amazing, hilarious #nephew...\n",
       "...                                                  ...\n",
       "49155  thought factory: left-right polarisation! #tru...\n",
       "49156  feeling like a mermaid ð #hairflip #neverre...\n",
       "49157  #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49158  happy, at work conference: right mindset leads...\n",
       "49159  my   song \"so glad\" free download!  #shoegaze ...\n",
       "\n",
       "[17197 rows x 1 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17197 entries, 31963 to 49159\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   17197 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 268.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение датасета на train и val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 2), (6393, 2))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df_train, \n",
    "                                    test_size=0.2, \n",
    "                                    random_state=10, \n",
    "                                    stratify=df_train['label'])\n",
    "\n",
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = set(punctuation)\n",
    "# Не будем очищать текст от апострофов, заменим их потом на пробелы,\n",
    "# т.к. встроенные в nltk английские стоп-слова и так потом отфильтруют лишнее\n",
    "puncts = puncts - {\"'\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Relict/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Relict/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = ''.join(char for char in txt if char not in puncts) # очистка от пунктуации\n",
    "    txt = txt.replace(\"'\", \" \")\n",
    "    txt = txt.lower().split()\n",
    "    txt = [word for word in txt if word.isalpha()] # очистка от символов и цифр\n",
    "    txt = [lemmatizer.lemmatize(word) for word in txt] # лемматизация\n",
    "    txt = [word for word in txt if word not in stopwords.words('english')] # очистка от стопслов\n",
    "    return ' '.join(txt)\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].apply(preprocess_text)\n",
    "df_val['tweet'] = df_val['tweet'].apply(preprocess_text)\n",
    "df_test['tweet'] = df_test['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28599</th>\n",
       "      <td>0</td>\n",
       "      <td>attending user user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27507</th>\n",
       "      <td>0</td>\n",
       "      <td>nimwit libtards embrace crookedhillary cronyga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16242</th>\n",
       "      <td>0</td>\n",
       "      <td>still sad pray today boanoite triste instagram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23285</th>\n",
       "      <td>0</td>\n",
       "      <td>user le month till york host one greatest annu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19694</th>\n",
       "      <td>0</td>\n",
       "      <td>user user user person change way living listen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "id                                                             \n",
       "28599      0                                attending user user\n",
       "27507      0  nimwit libtards embrace crookedhillary cronyga...\n",
       "16242      0  still sad pray today boanoite triste instagram...\n",
       "23285      0  user le month till york host one greatest annu...\n",
       "19694      0  user user user person change way living listen..."
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ''.join(df_train['tweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attending',\n",
       " 'user',\n",
       " 'usernimwit',\n",
       " 'libtards',\n",
       " 'embrace',\n",
       " 'crookedhillary',\n",
       " 'cronygarchy',\n",
       " 'braincell',\n",
       " 'anemicstill',\n",
       " 'sad']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(train_corpus)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 2000\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'day', 'love', 'u', 'amp', 'like', 'life', 'happy', 'get', 'wa']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = FreqDist(tokens)\n",
    "tokens_top = [items[0] for items in dist.most_common(MAX_WORDS - 1)]\n",
    "\n",
    "tokens_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 1,\n",
       " 'day': 2,\n",
       " 'love': 3,\n",
       " 'u': 4,\n",
       " 'amp': 5,\n",
       " 'like': 6,\n",
       " 'life': 7,\n",
       " 'happy': 8,\n",
       " 'get': 9,\n",
       " 'wa': 10,\n",
       " 'today': 11,\n",
       " 'new': 12,\n",
       " 'people': 13,\n",
       " 'time': 14,\n",
       " 'make': 15,\n",
       " 'father': 16,\n",
       " 'one': 17,\n",
       " 'see': 18,\n",
       " 'take': 19,\n",
       " 'go': 20,\n",
       " 'good': 21,\n",
       " 'smile': 22,\n",
       " 'work': 23,\n",
       " 'friend': 24,\n",
       " 'bihday': 25,\n",
       " 'ha': 26,\n",
       " 'need': 27,\n",
       " 'bull': 28,\n",
       " 'want': 29,\n",
       " 'girl': 30,\n",
       " 'positive': 31,\n",
       " 'weekend': 32,\n",
       " 'thankful': 33,\n",
       " 'way': 34,\n",
       " 'family': 35,\n",
       " 'week': 36,\n",
       " 'summer': 37,\n",
       " 'useruser': 38,\n",
       " 'year': 39,\n",
       " 'got': 40,\n",
       " 'great': 41,\n",
       " 'thing': 42,\n",
       " 'healthy': 43,\n",
       " 'morning': 44,\n",
       " 'home': 45,\n",
       " 'fun': 46,\n",
       " 'really': 47,\n",
       " 'best': 48,\n",
       " 'going': 49,\n",
       " 'back': 50,\n",
       " 'come': 51,\n",
       " 'friday': 52,\n",
       " 'first': 53,\n",
       " 'know': 54,\n",
       " 'blog': 55,\n",
       " 'beautiful': 56,\n",
       " 'world': 57,\n",
       " 'feel': 58,\n",
       " 'music': 59,\n",
       " 'think': 60,\n",
       " 'dad': 61,\n",
       " 'cute': 62,\n",
       " 'night': 63,\n",
       " 'never': 64,\n",
       " 'look': 65,\n",
       " 'tomorrow': 66,\n",
       " 'right': 67,\n",
       " 'say': 68,\n",
       " 'much': 69,\n",
       " 'trump': 70,\n",
       " 'wait': 71,\n",
       " 'iam': 72,\n",
       " 'silver': 73,\n",
       " 'gold': 74,\n",
       " 'sad': 75,\n",
       " 'orlando': 76,\n",
       " 'next': 77,\n",
       " 'even': 78,\n",
       " 'show': 79,\n",
       " 'sunday': 80,\n",
       " 'happiness': 81,\n",
       " 'find': 82,\n",
       " 'via': 83,\n",
       " 'still': 84,\n",
       " 'live': 85,\n",
       " 'well': 86,\n",
       " 'last': 87,\n",
       " 'altwaystoheal': 88,\n",
       " 'would': 89,\n",
       " 'fathersday': 90,\n",
       " 'old': 91,\n",
       " 'let': 92,\n",
       " 'woman': 93,\n",
       " 'game': 94,\n",
       " 'amazing': 95,\n",
       " 'follow': 96,\n",
       " 'thanks': 97,\n",
       " 'guy': 98,\n",
       " 'free': 99,\n",
       " 'climb': 100,\n",
       " 'tonight': 101,\n",
       " 'man': 102,\n",
       " 'bear': 103,\n",
       " 'always': 104,\n",
       " 'ready': 105,\n",
       " 'little': 106,\n",
       " 'selfie': 107,\n",
       " 'many': 108,\n",
       " 'whatever': 109,\n",
       " 'pay': 110,\n",
       " 'city': 111,\n",
       " 'dog': 112,\n",
       " 'quote': 113,\n",
       " 'na': 114,\n",
       " 'kid': 115,\n",
       " 'god': 116,\n",
       " 'ever': 117,\n",
       " 'everyone': 118,\n",
       " 'tweet': 119,\n",
       " 'help': 120,\n",
       " 'stop': 121,\n",
       " 'polar': 122,\n",
       " 'thank': 123,\n",
       " 'black': 124,\n",
       " 'made': 125,\n",
       " 'keep': 126,\n",
       " 'someone': 127,\n",
       " 'white': 128,\n",
       " 'looking': 129,\n",
       " 'every': 130,\n",
       " 'school': 131,\n",
       " 'hate': 132,\n",
       " 'child': 133,\n",
       " 'video': 134,\n",
       " 'sta': 135,\n",
       " 'food': 136,\n",
       " 'might': 137,\n",
       " 'feeling': 138,\n",
       " 'sun': 139,\n",
       " 'baby': 140,\n",
       " 'place': 141,\n",
       " 'dominate': 142,\n",
       " 'direct': 143,\n",
       " 'getting': 144,\n",
       " 'hope': 145,\n",
       " 'coming': 146,\n",
       " 'change': 147,\n",
       " 'watch': 148,\n",
       " 'around': 149,\n",
       " 'boy': 150,\n",
       " 'news': 151,\n",
       " 'instagood': 152,\n",
       " 'another': 153,\n",
       " 'may': 154,\n",
       " 'tear': 155,\n",
       " 'angry': 156,\n",
       " 'enjoy': 157,\n",
       " 'better': 158,\n",
       " 'hour': 159,\n",
       " 'funny': 160,\n",
       " 'peace': 161,\n",
       " 'wish': 162,\n",
       " 'holiday': 163,\n",
       " 'attack': 164,\n",
       " 'forward': 165,\n",
       " 'thought': 166,\n",
       " 'found': 167,\n",
       " 'saturday': 168,\n",
       " 'healing': 169,\n",
       " 'give': 170,\n",
       " 'blessed': 171,\n",
       " 'big': 172,\n",
       " 'lot': 173,\n",
       " 'team': 174,\n",
       " 'nothing': 175,\n",
       " 'believe': 176,\n",
       " 'done': 177,\n",
       " 'excited': 178,\n",
       " 'play': 179,\n",
       " 'libtard': 180,\n",
       " 'month': 181,\n",
       " 'finally': 182,\n",
       " 'photo': 183,\n",
       " 'without': 184,\n",
       " 'please': 185,\n",
       " 'away': 186,\n",
       " 'america': 187,\n",
       " 'wedding': 188,\n",
       " 'racing': 189,\n",
       " 'bing': 190,\n",
       " 'book': 191,\n",
       " 'cool': 192,\n",
       " 'two': 193,\n",
       " 'beach': 194,\n",
       " 'watching': 195,\n",
       " 'travel': 196,\n",
       " 'shooting': 197,\n",
       " 'word': 198,\n",
       " 'real': 199,\n",
       " 'something': 200,\n",
       " 'end': 201,\n",
       " 'face': 202,\n",
       " 'job': 203,\n",
       " 'long': 204,\n",
       " 'fan': 205,\n",
       " 'win': 206,\n",
       " 'awesome': 207,\n",
       " 'strong': 208,\n",
       " 'miss': 209,\n",
       " 'gun': 210,\n",
       " 'racist': 211,\n",
       " 'moment': 212,\n",
       " 'lost': 213,\n",
       " 'gorilla': 214,\n",
       " 'monday': 215,\n",
       " 'bad': 216,\n",
       " 'yeah': 217,\n",
       " 'proud': 218,\n",
       " 'gay': 219,\n",
       " 'left': 220,\n",
       " 'photooftheday': 221,\n",
       " 'buffalo': 222,\n",
       " 'ur': 223,\n",
       " 'yes': 224,\n",
       " 'n': 225,\n",
       " 'fashion': 226,\n",
       " 'person': 227,\n",
       " 'dream': 228,\n",
       " 'could': 229,\n",
       " 'soon': 230,\n",
       " 'money': 231,\n",
       " 'nice': 232,\n",
       " 'living': 233,\n",
       " 'call': 234,\n",
       " 'environment': 235,\n",
       " 'lovely': 236,\n",
       " 'r': 237,\n",
       " 'sexy': 238,\n",
       " 'mind': 239,\n",
       " 'said': 240,\n",
       " 'leave': 241,\n",
       " 'making': 242,\n",
       " 'hea': 243,\n",
       " 'story': 244,\n",
       " 'depression': 245,\n",
       " 'young': 246,\n",
       " 'lol': 247,\n",
       " 'hard': 248,\n",
       " 'everything': 249,\n",
       " 'try': 250,\n",
       " 'body': 251,\n",
       " 'stay': 252,\n",
       " 'simulator': 253,\n",
       " 'adapt': 254,\n",
       " 'mean': 255,\n",
       " 'head': 256,\n",
       " 'twitter': 257,\n",
       " 'cat': 258,\n",
       " 'affirmationuser': 259,\n",
       " 'timeuser': 260,\n",
       " 'success': 261,\n",
       " 'gon': 262,\n",
       " 'since': 263,\n",
       " 'hair': 264,\n",
       " 'liberal': 265,\n",
       " 'loveuser': 266,\n",
       " 'listen': 267,\n",
       " 'doe': 268,\n",
       " 'tbt': 269,\n",
       " 'poetry': 270,\n",
       " 'hot': 271,\n",
       " 'obama': 272,\n",
       " 'open': 273,\n",
       " 'yet': 274,\n",
       " 'conference': 275,\n",
       " 'post': 276,\n",
       " 'flower': 277,\n",
       " 'pa': 278,\n",
       " 'waiting': 279,\n",
       " 'full': 280,\n",
       " 'country': 281,\n",
       " 'simulation': 282,\n",
       " 'sleep': 283,\n",
       " 'im': 284,\n",
       " 'w': 285,\n",
       " 'positiveuser': 286,\n",
       " 'shop': 287,\n",
       " 'movie': 288,\n",
       " 'mom': 289,\n",
       " 'true': 290,\n",
       " 'june': 291,\n",
       " 'loved': 292,\n",
       " 'season': 293,\n",
       " 'men': 294,\n",
       " 'couple': 295,\n",
       " 'run': 296,\n",
       " 'health': 297,\n",
       " 'read': 298,\n",
       " 'house': 299,\n",
       " 'buy': 300,\n",
       " 'porn': 301,\n",
       " 'ticket': 302,\n",
       " 'gift': 303,\n",
       " 'care': 304,\n",
       " 'inspiration': 305,\n",
       " 'business': 306,\n",
       " 'delete': 307,\n",
       " 'son': 308,\n",
       " 'dayuser': 309,\n",
       " 'joy': 310,\n",
       " 'song': 311,\n",
       " 'tell': 312,\n",
       " 'cry': 313,\n",
       " 'meet': 314,\n",
       " 'v': 315,\n",
       " 'must': 316,\n",
       " 'grateful': 317,\n",
       " 'dead': 318,\n",
       " 'put': 319,\n",
       " 'race': 320,\n",
       " 'high': 321,\n",
       " 'trip': 322,\n",
       " 'seeing': 323,\n",
       " 'also': 324,\n",
       " 'thursday': 325,\n",
       " 'fact': 326,\n",
       " 'till': 327,\n",
       " 'already': 328,\n",
       " 'others': 329,\n",
       " 'useful': 330,\n",
       " 'cold': 331,\n",
       " 'nude': 332,\n",
       " 'enough': 333,\n",
       " 'american': 334,\n",
       " 'pretty': 335,\n",
       " 'beauty': 336,\n",
       " 'wonderful': 337,\n",
       " 'ppl': 338,\n",
       " 'lead': 339,\n",
       " 'use': 340,\n",
       " 'state': 341,\n",
       " 'blue': 342,\n",
       " 'event': 343,\n",
       " 'b': 344,\n",
       " 'truth': 345,\n",
       " 'shit': 346,\n",
       " 'share': 347,\n",
       " 'mood': 348,\n",
       " 'bird': 349,\n",
       " 'trying': 350,\n",
       " 'working': 351,\n",
       " 'anything': 352,\n",
       " 'empty': 353,\n",
       " 'alone': 354,\n",
       " 'london': 355,\n",
       " 'motivation': 356,\n",
       " 'fitness': 357,\n",
       " 'dance': 358,\n",
       " 'check': 359,\n",
       " 'car': 360,\n",
       " 'kind': 361,\n",
       " 'power': 362,\n",
       " 'drink': 363,\n",
       " 'mindset': 364,\n",
       " 'oh': 365,\n",
       " 'medium': 366,\n",
       " 'perfect': 367,\n",
       " 'hear': 368,\n",
       " 'rest': 369,\n",
       " 'super': 370,\n",
       " 'bed': 371,\n",
       " 'x': 372,\n",
       " 'wow': 373,\n",
       " 'social': 374,\n",
       " 'rip': 375,\n",
       " 'fuck': 376,\n",
       " 'anxiety': 377,\n",
       " 'gone': 378,\n",
       " 'running': 379,\n",
       " 'complete': 380,\n",
       " 'pic': 381,\n",
       " 'reason': 382,\n",
       " 'hand': 383,\n",
       " 'single': 384,\n",
       " 'together': 385,\n",
       " 'came': 386,\n",
       " 'allahsoil': 387,\n",
       " 'coffee': 388,\n",
       " 'favorite': 389,\n",
       " 'sex': 390,\n",
       " 'point': 391,\n",
       " 'retweet': 392,\n",
       " 'talk': 393,\n",
       " 'matter': 394,\n",
       " 'far': 395,\n",
       " 'victim': 396,\n",
       " 'vine': 397,\n",
       " 'sjw': 398,\n",
       " 'whole': 399,\n",
       " 'daddy': 400,\n",
       " 'kill': 401,\n",
       " 'visit': 402,\n",
       " 'gym': 403,\n",
       " 'laugh': 404,\n",
       " 'went': 405,\n",
       " 'bong': 406,\n",
       " 'hit': 407,\n",
       " 'sweet': 408,\n",
       " 'evening': 409,\n",
       " 'daughter': 410,\n",
       " 'target': 411,\n",
       " 'anyone': 412,\n",
       " 'remember': 413,\n",
       " 'organization': 414,\n",
       " 'else': 415,\n",
       " 'usa': 416,\n",
       " 'muslim': 417,\n",
       " 'vacation': 418,\n",
       " 'actually': 419,\n",
       " 'forever': 420,\n",
       " 'set': 421,\n",
       " 'join': 422,\n",
       " 'stas': 423,\n",
       " 'picture': 424,\n",
       " 'killed': 425,\n",
       " 'wrong': 426,\n",
       " 'comment': 427,\n",
       " 'crazy': 428,\n",
       " 'cause': 429,\n",
       " 'death': 430,\n",
       " 'fucking': 431,\n",
       " 'toptags': 432,\n",
       " 'lgbt': 433,\n",
       " 'group': 434,\n",
       " 'yesterday': 435,\n",
       " 'prayer': 436,\n",
       " 'tired': 437,\n",
       " 'loving': 438,\n",
       " 'eye': 439,\n",
       " 'become': 440,\n",
       " 'almost': 441,\n",
       " 'talking': 442,\n",
       " 'hu': 443,\n",
       " 'animal': 444,\n",
       " 'list': 445,\n",
       " 'forexuser': 446,\n",
       " 'playing': 447,\n",
       " 'sunshine': 448,\n",
       " 'le': 449,\n",
       " 'special': 450,\n",
       " 'relax': 451,\n",
       " 'red': 452,\n",
       " 'saying': 453,\n",
       " 'student': 454,\n",
       " 'followme': 455,\n",
       " 'episode': 456,\n",
       " 'minute': 457,\n",
       " 'cultureofdevelopment': 458,\n",
       " 'tuesday': 459,\n",
       " 'lady': 460,\n",
       " 'wednesday': 461,\n",
       " 'color': 462,\n",
       " 'racism': 463,\n",
       " 'tv': 464,\n",
       " 'used': 465,\n",
       " 'able': 466,\n",
       " 'rain': 467,\n",
       " 'break': 468,\n",
       " 'omg': 469,\n",
       " 'sick': 470,\n",
       " 'seen': 471,\n",
       " 'late': 472,\n",
       " 'human': 473,\n",
       " 'yay': 474,\n",
       " 'bit': 475,\n",
       " 'early': 476,\n",
       " 'dont': 477,\n",
       " 'style': 478,\n",
       " 'sure': 479,\n",
       " 'customer': 480,\n",
       " 'side': 481,\n",
       " 'goodmorning': 482,\n",
       " 'reach': 483,\n",
       " 'name': 484,\n",
       " 'despite': 485,\n",
       " 'sale': 486,\n",
       " 'parent': 487,\n",
       " 'light': 488,\n",
       " 'celebrate': 489,\n",
       " 'order': 490,\n",
       " 'lifestyle': 491,\n",
       " 'hill': 492,\n",
       " 'everyday': 493,\n",
       " 'vicinity': 494,\n",
       " 'follower': 495,\n",
       " 'nature': 496,\n",
       " 'future': 497,\n",
       " 'reading': 498,\n",
       " 'puppy': 499,\n",
       " 'suppo': 500,\n",
       " 'survive': 501,\n",
       " 'youtube': 502,\n",
       " 'lunch': 503,\n",
       " 'slut': 504,\n",
       " 'meeting': 505,\n",
       " 'walk': 506,\n",
       " 'arrived': 507,\n",
       " 'daily': 508,\n",
       " 'impoant': 509,\n",
       " 'shopping': 510,\n",
       " 'green': 511,\n",
       " 'mother': 512,\n",
       " 'nasty': 513,\n",
       " 'bring': 514,\n",
       " 'sunny': 515,\n",
       " 'anymore': 516,\n",
       " 'goal': 517,\n",
       " 'gbp': 518,\n",
       " 'staing': 519,\n",
       " 'nervous': 520,\n",
       " 'mountain': 521,\n",
       " 'freedom': 522,\n",
       " 'club': 523,\n",
       " 'final': 524,\n",
       " 'account': 525,\n",
       " 'tragedy': 526,\n",
       " 'sister': 527,\n",
       " 'garden': 528,\n",
       " 'understand': 529,\n",
       " 'design': 530,\n",
       " 'lifeuser': 531,\n",
       " 'ago': 532,\n",
       " 'saw': 533,\n",
       " 'sign': 534,\n",
       " 'instagram': 535,\n",
       " 'blur': 536,\n",
       " 'close': 537,\n",
       " 'history': 538,\n",
       " 'stomping': 539,\n",
       " 'present': 540,\n",
       " 'prayfororlando': 541,\n",
       " 'lucky': 542,\n",
       " 'reached': 543,\n",
       " 'actor': 544,\n",
       " 'memory': 545,\n",
       " 'la': 546,\n",
       " 'date': 547,\n",
       " 'act': 548,\n",
       " 'uk': 549,\n",
       " 'happened': 550,\n",
       " 'flight': 551,\n",
       " 'called': 552,\n",
       " 'dj': 553,\n",
       " 'park': 554,\n",
       " 'shot': 555,\n",
       " 'update': 556,\n",
       " 'depressed': 557,\n",
       " 'task': 558,\n",
       " 'star': 559,\n",
       " 'film': 560,\n",
       " 'line': 561,\n",
       " 'idea': 562,\n",
       " 'phone': 563,\n",
       " 'least': 564,\n",
       " 'service': 565,\n",
       " 'smiling': 566,\n",
       " 'gt': 567,\n",
       " 'nyc': 568,\n",
       " 'snapchat': 569,\n",
       " 'president': 570,\n",
       " 'brother': 571,\n",
       " 'boyfriend': 572,\n",
       " 'app': 573,\n",
       " 'problem': 574,\n",
       " 'suppoers': 575,\n",
       " 'rather': 576,\n",
       " 'eat': 577,\n",
       " 'later': 578,\n",
       " 'half': 579,\n",
       " 'choice': 580,\n",
       " 'pain': 581,\n",
       " 'brexit': 582,\n",
       " 'learn': 583,\n",
       " 'fly': 584,\n",
       " 'fresh': 585,\n",
       " 'second': 586,\n",
       " 'local': 587,\n",
       " 'view': 588,\n",
       " 'naughty': 589,\n",
       " 'control': 590,\n",
       " 'horny': 591,\n",
       " 'rock': 592,\n",
       " 'sorry': 593,\n",
       " 'police': 594,\n",
       " 'along': 595,\n",
       " 'soul': 596,\n",
       " 'booked': 597,\n",
       " 'happen': 598,\n",
       " 'issue': 599,\n",
       " 'football': 600,\n",
       " 'education': 601,\n",
       " 'leadership': 602,\n",
       " 'conce': 603,\n",
       " 'photography': 604,\n",
       " 'shy': 605,\n",
       " 'fall': 606,\n",
       " 'truly': 607,\n",
       " 'pray': 608,\n",
       " 'sunset': 609,\n",
       " 'afternoon': 610,\n",
       " 'camp': 611,\n",
       " 'bc': 612,\n",
       " 'tgif': 613,\n",
       " 'happening': 614,\n",
       " 'plan': 615,\n",
       " 'england': 616,\n",
       " 'question': 617,\n",
       " 'past': 618,\n",
       " 'sound': 619,\n",
       " 'bless': 620,\n",
       " 'disney': 621,\n",
       " 'heal': 622,\n",
       " 'yoga': 623,\n",
       " 'chase': 624,\n",
       " 'seems': 625,\n",
       " 'choose': 626,\n",
       " 'violence': 627,\n",
       " 'guess': 628,\n",
       " 'though': 629,\n",
       " 'move': 630,\n",
       " 'vote': 631,\n",
       " 'can': 632,\n",
       " 'not': 633,\n",
       " 'stuff': 634,\n",
       " 'easy': 635,\n",
       " 'street': 636,\n",
       " 'india': 637,\n",
       " 'joke': 638,\n",
       " 'broken': 639,\n",
       " 'woh': 640,\n",
       " 'lover': 641,\n",
       " 'model': 642,\n",
       " 'florida': 643,\n",
       " 'kinky': 644,\n",
       " 'wet': 645,\n",
       " 'turn': 646,\n",
       " 'maybe': 647,\n",
       " 'test': 648,\n",
       " 'water': 649,\n",
       " 'hero': 650,\n",
       " 'folk': 651,\n",
       " 'picoftheday': 652,\n",
       " 'sho': 653,\n",
       " 'cant': 654,\n",
       " 'gop': 655,\n",
       " 'politics': 656,\n",
       " 'begin': 657,\n",
       " 'wife': 658,\n",
       " 'different': 659,\n",
       " 'cake': 660,\n",
       " 'course': 661,\n",
       " 'wan': 662,\n",
       " 'ride': 663,\n",
       " 'hatred': 664,\n",
       " 'safe': 665,\n",
       " 'husband': 666,\n",
       " 'sky': 667,\n",
       " 'mad': 668,\n",
       " 'female': 669,\n",
       " 'wanted': 670,\n",
       " 'product': 671,\n",
       " 'mass': 672,\n",
       " 'lose': 673,\n",
       " 'queen': 674,\n",
       " 'friendship': 675,\n",
       " 'class': 676,\n",
       " 'worst': 677,\n",
       " 'thinking': 678,\n",
       " 'happier': 679,\n",
       " 'training': 680,\n",
       " 'asian': 681,\n",
       " 'needed': 682,\n",
       " 'dear': 683,\n",
       " 'community': 684,\n",
       " 'review': 685,\n",
       " 'member': 686,\n",
       " 'launch': 687,\n",
       " 'gave': 688,\n",
       " 'breakfast': 689,\n",
       " 'lie': 690,\n",
       " 'online': 691,\n",
       " 'giving': 692,\n",
       " 'using': 693,\n",
       " 'behind': 694,\n",
       " 'workout': 695,\n",
       " 'bought': 696,\n",
       " 'yr': 697,\n",
       " 'makeup': 698,\n",
       " 'dark': 699,\n",
       " 'action': 700,\n",
       " 'forget': 701,\n",
       " 'euro': 702,\n",
       " 'shoe': 703,\n",
       " 'surprise': 704,\n",
       " 'vast': 705,\n",
       " 'expanse': 706,\n",
       " 'mine': 707,\n",
       " 'hillary': 708,\n",
       " 'card': 709,\n",
       " 'anniversary': 710,\n",
       " 'latest': 711,\n",
       " 'war': 712,\n",
       " 'finger': 713,\n",
       " 'ahead': 714,\n",
       " 'heard': 715,\n",
       " 'celebration': 716,\n",
       " 'top': 717,\n",
       " 'bag': 718,\n",
       " 'ask': 719,\n",
       " 'speak': 720,\n",
       " 'beat': 721,\n",
       " 'france': 722,\n",
       " 'flag': 723,\n",
       " 'xxx': 724,\n",
       " 'oil': 725,\n",
       " 'wake': 726,\n",
       " 'experience': 727,\n",
       " 'teen': 728,\n",
       " 'series': 729,\n",
       " 'taking': 730,\n",
       " 'died': 731,\n",
       " 'adventure': 732,\n",
       " 'lonely': 733,\n",
       " 'client': 734,\n",
       " 'step': 735,\n",
       " 'spos': 736,\n",
       " 'beer': 737,\n",
       " 'college': 738,\n",
       " 'bitch': 739,\n",
       " 'case': 740,\n",
       " 'nail': 741,\n",
       " 'fantastic': 742,\n",
       " 'political': 743,\n",
       " 'award': 744,\n",
       " 'festival': 745,\n",
       " 'stupid': 746,\n",
       " 'sea': 747,\n",
       " 'republican': 748,\n",
       " 'healthyuser': 749,\n",
       " 'room': 750,\n",
       " 'dinner': 751,\n",
       " 'bday': 752,\n",
       " 'office': 753,\n",
       " 'low': 754,\n",
       " 'due': 755,\n",
       " 'smileuser': 756,\n",
       " 'newyork': 757,\n",
       " 'bike': 758,\n",
       " 'nation': 759,\n",
       " 'source': 760,\n",
       " 'writing': 761,\n",
       " 'welcome': 762,\n",
       " 'culture': 763,\n",
       " 'treat': 764,\n",
       " 'glad': 765,\n",
       " 'tip': 766,\n",
       " 'ok': 767,\n",
       " 'page': 768,\n",
       " 'message': 769,\n",
       " 'moving': 770,\n",
       " 'pathetic': 771,\n",
       " 'session': 772,\n",
       " 'small': 773,\n",
       " 'fit': 774,\n",
       " 'til': 775,\n",
       " 'staff': 776,\n",
       " 'towards': 777,\n",
       " 'rooster': 778,\n",
       " 'opening': 779,\n",
       " 'send': 780,\n",
       " 'return': 781,\n",
       " 'law': 782,\n",
       " 'national': 783,\n",
       " 'eah': 784,\n",
       " 'inshot': 785,\n",
       " 'leaving': 786,\n",
       " 'website': 787,\n",
       " 'cover': 788,\n",
       " 'tea': 789,\n",
       " 'deal': 790,\n",
       " 'content': 791,\n",
       " 'hardcore': 792,\n",
       " 'especially': 793,\n",
       " 'calling': 794,\n",
       " 'met': 795,\n",
       " 'probably': 796,\n",
       " 'told': 797,\n",
       " 'hell': 798,\n",
       " 'vibe': 799,\n",
       " 'pet': 800,\n",
       " 'respect': 801,\n",
       " 'loss': 802,\n",
       " 'voice': 803,\n",
       " 'king': 804,\n",
       " 'trust': 805,\n",
       " 'longer': 806,\n",
       " 'release': 807,\n",
       " 'relationship': 808,\n",
       " 'poor': 809,\n",
       " 'luck': 810,\n",
       " 'reality': 811,\n",
       " 'die': 812,\n",
       " 'company': 813,\n",
       " 'blonde': 814,\n",
       " 'instead': 815,\n",
       " 'campaign': 816,\n",
       " 'fear': 817,\n",
       " 'oitnb': 818,\n",
       " 'self': 819,\n",
       " 'stand': 820,\n",
       " 'pool': 821,\n",
       " 'million': 822,\n",
       " 'album': 823,\n",
       " 'road': 824,\n",
       " 'simple': 825,\n",
       " 'ta': 826,\n",
       " 'nba': 827,\n",
       " 'terrorist': 828,\n",
       " 'project': 829,\n",
       " 'damn': 830,\n",
       " 'finding': 831,\n",
       " 'pink': 832,\n",
       " 'wine': 833,\n",
       " 'fire': 834,\n",
       " 'three': 835,\n",
       " 'decision': 836,\n",
       " 'result': 837,\n",
       " 'chance': 838,\n",
       " 'pizza': 839,\n",
       " 'e': 840,\n",
       " 'benefit': 841,\n",
       " 'officially': 842,\n",
       " 'tragic': 843,\n",
       " 'player': 844,\n",
       " 'missing': 845,\n",
       " 'spend': 846,\n",
       " 'trending': 847,\n",
       " 'entire': 848,\n",
       " 'agree': 849,\n",
       " 'pride': 850,\n",
       " 'dress': 851,\n",
       " 'church': 852,\n",
       " 'official': 853,\n",
       " 'busy': 854,\n",
       " 'july': 855,\n",
       " 'fight': 856,\n",
       " 'internet': 857,\n",
       " 'weather': 858,\n",
       " 'analytics': 859,\n",
       " 'diet': 860,\n",
       " 'add': 861,\n",
       " 'congrats': 862,\n",
       " 'ramadan': 863,\n",
       " 'thankyou': 864,\n",
       " 'available': 865,\n",
       " 'previous': 866,\n",
       " 'haha': 867,\n",
       " 'p': 868,\n",
       " 'yummy': 869,\n",
       " 'lawofattraction': 870,\n",
       " 'number': 871,\n",
       " 'emotion': 872,\n",
       " 'aist': 873,\n",
       " 'huge': 874,\n",
       " 'literally': 875,\n",
       " 'temple': 876,\n",
       " 'paid': 877,\n",
       " 'tool': 878,\n",
       " 'given': 879,\n",
       " 'mr': 880,\n",
       " 'donald': 881,\n",
       " 'bride': 882,\n",
       " 'snapshot': 883,\n",
       " 'took': 884,\n",
       " 'meme': 885,\n",
       " 'married': 886,\n",
       " 'instalike': 887,\n",
       " 'absolutely': 888,\n",
       " 'instamood': 889,\n",
       " 'planning': 890,\n",
       " 'passed': 891,\n",
       " 'instadaily': 892,\n",
       " 'userthankful': 893,\n",
       " 'age': 894,\n",
       " 'politician': 895,\n",
       " 'space': 896,\n",
       " 'punjab': 897,\n",
       " 'wisdom': 898,\n",
       " 'foot': 899,\n",
       " 'cut': 900,\n",
       " 'melancholy': 901,\n",
       " 'tour': 902,\n",
       " 'wonder': 903,\n",
       " 'idiot': 904,\n",
       " 'missed': 905,\n",
       " 'hello': 906,\n",
       " 'wishing': 907,\n",
       " 'pop': 908,\n",
       " 'pulse': 909,\n",
       " 'da': 910,\n",
       " 'nigga': 911,\n",
       " 'horrible': 912,\n",
       " 'sense': 913,\n",
       " 'hey': 914,\n",
       " 'gorgeous': 915,\n",
       " 'outside': 916,\n",
       " 'essentialoils': 917,\n",
       " 'todayuser': 918,\n",
       " 'marriage': 919,\n",
       " 'finished': 920,\n",
       " 'blogger': 921,\n",
       " 'favourite': 922,\n",
       " 'heabroken': 923,\n",
       " 'totally': 924,\n",
       " 'staed': 925,\n",
       " 'pig': 926,\n",
       " 'singing': 927,\n",
       " 'count': 928,\n",
       " 'faith': 929,\n",
       " 'train': 930,\n",
       " 'piece': 931,\n",
       " 'claim': 932,\n",
       " 'apple': 933,\n",
       " 'udtapunjab': 934,\n",
       " 'finish': 935,\n",
       " 'realize': 936,\n",
       " 'anime': 937,\n",
       " 'weekenduser': 938,\n",
       " 'adult': 939,\n",
       " 'allow': 940,\n",
       " 'door': 941,\n",
       " 'hotel': 942,\n",
       " 'email': 943,\n",
       " 'ice': 944,\n",
       " 'vega': 945,\n",
       " 'youth': 946,\n",
       " 'deep': 947,\n",
       " 'germany': 948,\n",
       " 'box': 949,\n",
       " 'york': 950,\n",
       " 'winner': 951,\n",
       " 'ff': 952,\n",
       " 'turned': 953,\n",
       " 'kick': 954,\n",
       " 'nightclub': 955,\n",
       " 'answer': 956,\n",
       " 'graduation': 957,\n",
       " 'bar': 958,\n",
       " 'scared': 959,\n",
       " 'sometimes': 960,\n",
       " 'grow': 961,\n",
       " 'ending': 962,\n",
       " 'dory': 963,\n",
       " 'nobody': 964,\n",
       " 'bill': 965,\n",
       " 'hug': 966,\n",
       " 'dancing': 967,\n",
       " 'broker': 968,\n",
       " 'key': 969,\n",
       " 'leader': 970,\n",
       " 'market': 971,\n",
       " 'vegan': 972,\n",
       " 'chill': 973,\n",
       " 'naked': 974,\n",
       " 'min': 975,\n",
       " 'singer': 976,\n",
       " 'offer': 977,\n",
       " 'lyric': 978,\n",
       " 'poem': 979,\n",
       " 'enjoying': 980,\n",
       " 'wantuser': 981,\n",
       " 'c': 982,\n",
       " 'pm': 983,\n",
       " 'public': 984,\n",
       " 'upset': 985,\n",
       " 'l': 986,\n",
       " 'everybody': 987,\n",
       " 'level': 988,\n",
       " 'bestie': 989,\n",
       " 'fast': 990,\n",
       " 'gif': 991,\n",
       " 'attitude': 992,\n",
       " 'rainbow': 993,\n",
       " 'christian': 994,\n",
       " 'deletetweetsuser': 995,\n",
       " 'lake': 996,\n",
       " 'strength': 997,\n",
       " 'definitely': 998,\n",
       " 'leakage': 999,\n",
       " 'calgary': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = {word: count for count, word in dict(enumerate(tokens_top, 1)).items()}\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переведём твиты в набор индексов, добавим паддинг\n",
    "\n",
    "def text_to_sequence(txt, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(txt)\n",
    "    for word in tokens:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "\n",
    "    padding = [0] * (maxlen-len(result))\n",
    "    return result[-maxlen:] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 20), (6393, 20))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_train['tweet'].values])\n",
    "X_val = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_val['tweet'].values])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем свёрточную нейросеть\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, embedding_dim=128, out_channel=64, num_classes=1, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n",
    "        self.conv_1 = nn.Conv1d(embedding_dim, out_channel, kernel_size=3, padding='same') \n",
    "        self.bn1 = nn.BatchNorm1d(out_channel)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_1 = nn.Linear(out_channel, num_classes)\n",
    "        self.dp1d = nn.Dropout1d(0.5)\n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):     # Для понимания обозначим размеры входных данных на каждом слое,\n",
    "                              # используя гиперпараметры по умолчанию и max_len=20\n",
    "        x = self.embedding(x) # (1, 20) -> (1, 20, 128)       \n",
    "        x = x.permute(0, 2, 1) # (1, 20, 128) -> (1, 128, 20)\n",
    "        x = self.conv_1(x) # (1, 128, 20) -> (1, 64, 20)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dp1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x) # (1, 64, 20) -> (1, 64, 10)\n",
    "        \n",
    "        x = torch.max(x, axis=2).values # (1, 64, 10) -> (1, 64)\n",
    "        x = self.dp(x)\n",
    "        x = self.linear_1(x) # (1, 64) -> (1, 1)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = torch.IntTensor(x).to(device)\n",
    "        x = self.forward(x)\n",
    "        x = torch.squeeze((x > self.threshold).int())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchinfo in c:\\users\\relict\\appdata\\roaming\\python\\python39\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [1, 1]                    --\n",
       "├─Embedding: 1-1                         [1, 20, 128]              256,000\n",
       "├─Conv1d: 1-2                            [1, 64, 20]               24,640\n",
       "├─BatchNorm1d: 1-3                       [1, 64, 20]               128\n",
       "├─Dropout1d: 1-4                         [1, 64, 20]               --\n",
       "├─ReLU: 1-5                              [1, 64, 20]               --\n",
       "├─MaxPool1d: 1-6                         [1, 64, 10]               --\n",
       "├─Dropout: 1-7                           [1, 64]                   --\n",
       "├─Linear: 1-8                            [1, 1]                    65\n",
       "==========================================================================================\n",
       "Total params: 280,833\n",
       "Trainable params: 280,833\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.75\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 1.12\n",
       "Estimated Total Size (MB): 1.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Net(), input_data=torch.IntTensor(X_train[np.newaxis, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.target = torch.from_numpy(target)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(10)\n",
    "\n",
    "train_dataset = DataWrapper(X_train, df_train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "val_dataset = DataWrapper(X_val, df_val['label'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(epochs, embedding_dim, hidden_size, lr, threshold=0.5, return_model=False):\n",
    "# def train_nn(epochs, embedding_dim, hidden_size=32, lr=0.001, threshold=0.5, return_model=False):\n",
    "    \n",
    "    torch.random.manual_seed(10)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    net = Net(vocab_size=MAX_WORDS, embedding_dim=embedding_dim, \n",
    "              out_channel=hidden_size, threshold=threshold).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = np.array([])\n",
    "        test_losses = np.array([])\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            net.train()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses = np.append(train_losses, loss.item())\n",
    "\n",
    "            net.eval()\n",
    "            outputs = torch.squeeze((net(inputs) > threshold).int())\n",
    "\n",
    "            tp += ((labels == 1) & (outputs == 1)).sum().item()\n",
    "            tn += ((labels == 0) & (outputs == 0)).sum().item()\n",
    "            fp += ((labels == 0) & (outputs == 1)).sum().item()\n",
    "            fn += ((labels == 1) & (outputs == 0)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
    "              f'Loss: {train_losses.mean():.3f}. ' \\\n",
    "              f'F1-score: {f1_score:.3f}', end='. ')\n",
    "\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "                test_losses = np.append(test_losses, loss.item())\n",
    "\n",
    "                tp += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                tn += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "                fp += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                fn += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        print(f'Test loss: {test_losses.mean():.3f}. Test F1-score: {f1_score:.3f}. Precision: {precision:.3f}. Recall: {recall:.3f}')\n",
    "\n",
    "    print('Training is finished!')\n",
    "    if return_model:\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Loss: 0.892. F1-score: 0.096. Test loss: 0.270. Test F1-score: 0.026. Precision: 0.076. Recall: 0.016\n",
      "Epoch [2/10]. Loss: 0.504. F1-score: 0.014. Test loss: 0.249. Test F1-score: 0.012. Precision: 0.042. Recall: 0.007\n",
      "Epoch [3/10]. Loss: 0.432. F1-score: 0.019. Test loss: 0.237. Test F1-score: 0.023. Precision: 0.081. Recall: 0.013\n",
      "Epoch [4/10]. Loss: 0.383. F1-score: 0.029. Test loss: 0.230. Test F1-score: 0.018. Precision: 1.000. Recall: 0.009\n",
      "Epoch [5/10]. Loss: 0.353. F1-score: 0.030. Test loss: 0.225. Test F1-score: 0.018. Precision: 1.000. Recall: 0.009\n",
      "Epoch [6/10]. Loss: 0.332. F1-score: 0.032. Test loss: 0.222. Test F1-score: 0.018. Precision: 1.000. Recall: 0.009\n",
      "Epoch [7/10]. Loss: 0.313. F1-score: 0.052. Test loss: 0.222. Test F1-score: 0.048. Precision: 1.000. Recall: 0.025\n",
      "Epoch [8/10]. Loss: 0.298. F1-score: 0.063. Test loss: 0.216. Test F1-score: 0.018. Precision: 1.000. Recall: 0.009\n",
      "Epoch [9/10]. Loss: 0.292. F1-score: 0.054. Test loss: 0.218. Test F1-score: 0.077. Precision: 1.000. Recall: 0.040\n",
      "Epoch [10/10]. Loss: 0.281. F1-score: 0.081. Test loss: 0.209. Test F1-score: 0.081. Precision: 1.000. Recall: 0.042\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(epochs=10, embedding_dim=64, hidden_size=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.892. F1-score: 0.096. Test loss: 0.270. Test F1-score: 0.026. Precision: 0.076. Recall: 0.016\n",
      "Epoch [2/50]. Loss: 0.504. F1-score: 0.014. Test loss: 0.249. Test F1-score: 0.012. Precision: 0.042. Recall: 0.007\n",
      "Epoch [3/50]. Loss: 0.432. F1-score: 0.019. Test loss: 0.237. Test F1-score: 0.023. Precision: 0.081. Recall: 0.013\n",
      "Epoch [4/50]. Loss: 0.383. F1-score: 0.029. Test loss: 0.230. Test F1-score: 0.018. Precision: 1.000. Recall: 0.009\n",
      "Epoch [5/50]. Loss: 0.353. F1-score: 0.030. Test loss: 0.225. Test F1-score: 0.018. Precision: 1.000. Recall: 0.009\n",
      "Epoch [6/50]. Loss: 0.332. F1-score: 0.032. Test loss: 0.222. Test F1-score: 0.018. Precision: 1.000. Recall: 0.009\n",
      "Epoch [7/50]. Loss: 0.313. F1-score: 0.052. Test loss: 0.222. Test F1-score: 0.048. Precision: 1.000. Recall: 0.025\n",
      "Epoch [8/50]. Loss: 0.298. F1-score: 0.063. Test loss: 0.216. Test F1-score: 0.018. Precision: 1.000. Recall: 0.009\n",
      "Epoch [9/50]. Loss: 0.292. F1-score: 0.054. Test loss: 0.218. Test F1-score: 0.077. Precision: 1.000. Recall: 0.040\n",
      "Epoch [10/50]. Loss: 0.281. F1-score: 0.081. Test loss: 0.209. Test F1-score: 0.081. Precision: 1.000. Recall: 0.042\n",
      "Epoch [11/50]. Loss: 0.268. F1-score: 0.049. Test loss: 0.207. Test F1-score: 0.048. Precision: 1.000. Recall: 0.025\n",
      "Epoch [12/50]. Loss: 0.263. F1-score: 0.059. Test loss: 0.207. Test F1-score: 0.065. Precision: 1.000. Recall: 0.033\n",
      "Epoch [13/50]. Loss: 0.257. F1-score: 0.057. Test loss: 0.204. Test F1-score: 0.069. Precision: 1.000. Recall: 0.036\n",
      "Epoch [14/50]. Loss: 0.251. F1-score: 0.081. Test loss: 0.204. Test F1-score: 0.090. Precision: 1.000. Recall: 0.047\n",
      "Epoch [15/50]. Loss: 0.241. F1-score: 0.068. Test loss: 0.199. Test F1-score: 0.081. Precision: 1.000. Recall: 0.042\n",
      "Epoch [16/50]. Loss: 0.240. F1-score: 0.062. Test loss: 0.198. Test F1-score: 0.110. Precision: 1.000. Recall: 0.058\n",
      "Epoch [17/50]. Loss: 0.233. F1-score: 0.072. Test loss: 0.194. Test F1-score: 0.114. Precision: 1.000. Recall: 0.060\n",
      "Epoch [18/50]. Loss: 0.228. F1-score: 0.110. Test loss: 0.194. Test F1-score: 0.102. Precision: 1.000. Recall: 0.054\n",
      "Epoch [19/50]. Loss: 0.227. F1-score: 0.093. Test loss: 0.194. Test F1-score: 0.102. Precision: 1.000. Recall: 0.054\n",
      "Epoch [20/50]. Loss: 0.219. F1-score: 0.111. Test loss: 0.190. Test F1-score: 0.114. Precision: 1.000. Recall: 0.060\n",
      "Epoch [21/50]. Loss: 0.217. F1-score: 0.113. Test loss: 0.186. Test F1-score: 0.114. Precision: 1.000. Recall: 0.060\n",
      "Epoch [22/50]. Loss: 0.212. F1-score: 0.116. Test loss: 0.187. Test F1-score: 0.118. Precision: 1.000. Recall: 0.062\n",
      "Epoch [23/50]. Loss: 0.206. F1-score: 0.125. Test loss: 0.186. Test F1-score: 0.126. Precision: 1.000. Recall: 0.067\n",
      "Epoch [24/50]. Loss: 0.203. F1-score: 0.149. Test loss: 0.185. Test F1-score: 0.153. Precision: 1.000. Recall: 0.083\n",
      "Epoch [25/50]. Loss: 0.199. F1-score: 0.178. Test loss: 0.181. Test F1-score: 0.171. Precision: 0.977. Recall: 0.094\n",
      "Epoch [26/50]. Loss: 0.194. F1-score: 0.206. Test loss: 0.179. Test F1-score: 0.208. Precision: 0.981. Recall: 0.116\n",
      "Epoch [27/50]. Loss: 0.193. F1-score: 0.232. Test loss: 0.182. Test F1-score: 0.222. Precision: 0.982. Recall: 0.125\n",
      "Epoch [28/50]. Loss: 0.190. F1-score: 0.243. Test loss: 0.180. Test F1-score: 0.246. Precision: 0.969. Recall: 0.141\n",
      "Epoch [29/50]. Loss: 0.183. F1-score: 0.268. Test loss: 0.175. Test F1-score: 0.276. Precision: 0.973. Recall: 0.161\n",
      "Epoch [30/50]. Loss: 0.178. F1-score: 0.315. Test loss: 0.176. Test F1-score: 0.305. Precision: 0.976. Recall: 0.181\n",
      "Epoch [31/50]. Loss: 0.176. F1-score: 0.330. Test loss: 0.175. Test F1-score: 0.311. Precision: 0.965. Recall: 0.185\n",
      "Epoch [32/50]. Loss: 0.173. F1-score: 0.361. Test loss: 0.173. Test F1-score: 0.331. Precision: 0.947. Recall: 0.201\n",
      "Epoch [33/50]. Loss: 0.172. F1-score: 0.384. Test loss: 0.172. Test F1-score: 0.357. Precision: 0.934. Recall: 0.221\n",
      "Epoch [34/50]. Loss: 0.167. F1-score: 0.403. Test loss: 0.173. Test F1-score: 0.378. Precision: 0.938. Recall: 0.237\n",
      "Epoch [35/50]. Loss: 0.164. F1-score: 0.438. Test loss: 0.172. Test F1-score: 0.391. Precision: 0.896. Recall: 0.250\n",
      "Epoch [36/50]. Loss: 0.162. F1-score: 0.437. Test loss: 0.171. Test F1-score: 0.391. Precision: 0.925. Recall: 0.248\n",
      "Epoch [37/50]. Loss: 0.159. F1-score: 0.453. Test loss: 0.170. Test F1-score: 0.405. Precision: 0.900. Recall: 0.261\n",
      "Epoch [38/50]. Loss: 0.159. F1-score: 0.465. Test loss: 0.173. Test F1-score: 0.415. Precision: 0.916. Recall: 0.268\n",
      "Epoch [39/50]. Loss: 0.159. F1-score: 0.463. Test loss: 0.172. Test F1-score: 0.412. Precision: 0.896. Recall: 0.268\n",
      "Epoch [40/50]. Loss: 0.154. F1-score: 0.478. Test loss: 0.170. Test F1-score: 0.420. Precision: 0.891. Recall: 0.275\n",
      "Epoch [41/50]. Loss: 0.155. F1-score: 0.499. Test loss: 0.170. Test F1-score: 0.432. Precision: 0.883. Recall: 0.286\n",
      "Epoch [42/50]. Loss: 0.148. F1-score: 0.505. Test loss: 0.173. Test F1-score: 0.433. Precision: 0.895. Recall: 0.286\n",
      "Epoch [43/50]. Loss: 0.147. F1-score: 0.518. Test loss: 0.172. Test F1-score: 0.442. Precision: 0.886. Recall: 0.295\n",
      "Epoch [44/50]. Loss: 0.145. F1-score: 0.525. Test loss: 0.172. Test F1-score: 0.453. Precision: 0.889. Recall: 0.304\n",
      "Epoch [45/50]. Loss: 0.144. F1-score: 0.535. Test loss: 0.171. Test F1-score: 0.459. Precision: 0.880. Recall: 0.310\n",
      "Epoch [46/50]. Loss: 0.143. F1-score: 0.542. Test loss: 0.175. Test F1-score: 0.452. Precision: 0.883. Recall: 0.304\n",
      "Epoch [47/50]. Loss: 0.144. F1-score: 0.541. Test loss: 0.170. Test F1-score: 0.464. Precision: 0.881. Recall: 0.315\n",
      "Epoch [48/50]. Loss: 0.140. F1-score: 0.546. Test loss: 0.173. Test F1-score: 0.456. Precision: 0.913. Recall: 0.304\n",
      "Epoch [49/50]. Loss: 0.141. F1-score: 0.537. Test loss: 0.178. Test F1-score: 0.462. Precision: 0.903. Recall: 0.310\n",
      "Epoch [50/50]. Loss: 0.138. F1-score: 0.549. Test loss: 0.173. Test F1-score: 0.468. Precision: 0.910. Recall: 0.315\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_1=train_nn(epochs=50, embedding_dim=64, hidden_size=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.658. F1-score: 0.091. Test loss: 0.242. Test F1-score: 0.093. Precision: 0.815. Recall: 0.049\n",
      "Epoch [2/50]. Loss: 0.393. F1-score: 0.121. Test loss: 0.223. Test F1-score: 0.129. Precision: 0.969. Recall: 0.069\n",
      "Epoch [3/50]. Loss: 0.330. F1-score: 0.145. Test loss: 0.214. Test F1-score: 0.144. Precision: 0.946. Recall: 0.078\n",
      "Epoch [4/50]. Loss: 0.301. F1-score: 0.156. Test loss: 0.207. Test F1-score: 0.144. Precision: 0.946. Recall: 0.078\n",
      "Epoch [5/50]. Loss: 0.275. F1-score: 0.166. Test loss: 0.202. Test F1-score: 0.141. Precision: 0.971. Recall: 0.076\n",
      "Epoch [6/50]. Loss: 0.264. F1-score: 0.175. Test loss: 0.199. Test F1-score: 0.160. Precision: 0.975. Recall: 0.087\n",
      "Epoch [7/50]. Loss: 0.247. F1-score: 0.184. Test loss: 0.195. Test F1-score: 0.156. Precision: 0.974. Recall: 0.085\n",
      "Epoch [8/50]. Loss: 0.238. F1-score: 0.192. Test loss: 0.191. Test F1-score: 0.182. Precision: 0.978. Recall: 0.100\n",
      "Epoch [9/50]. Loss: 0.233. F1-score: 0.216. Test loss: 0.186. Test F1-score: 0.200. Precision: 0.980. Recall: 0.112\n",
      "Epoch [10/50]. Loss: 0.225. F1-score: 0.229. Test loss: 0.186. Test F1-score: 0.206. Precision: 0.929. Recall: 0.116\n",
      "Epoch [11/50]. Loss: 0.218. F1-score: 0.252. Test loss: 0.182. Test F1-score: 0.234. Precision: 0.938. Recall: 0.134\n",
      "Epoch [12/50]. Loss: 0.210. F1-score: 0.281. Test loss: 0.179. Test F1-score: 0.254. Precision: 0.917. Recall: 0.147\n",
      "Epoch [13/50]. Loss: 0.205. F1-score: 0.307. Test loss: 0.179. Test F1-score: 0.277. Precision: 0.913. Recall: 0.163\n",
      "Epoch [14/50]. Loss: 0.201. F1-score: 0.335. Test loss: 0.177. Test F1-score: 0.304. Precision: 0.891. Recall: 0.183\n",
      "Epoch [15/50]. Loss: 0.196. F1-score: 0.359. Test loss: 0.174. Test F1-score: 0.312. Precision: 0.885. Recall: 0.190\n",
      "Epoch [16/50]. Loss: 0.192. F1-score: 0.389. Test loss: 0.172. Test F1-score: 0.332. Precision: 0.860. Recall: 0.205\n",
      "Epoch [17/50]. Loss: 0.186. F1-score: 0.399. Test loss: 0.171. Test F1-score: 0.344. Precision: 0.873. Recall: 0.214\n",
      "Epoch [18/50]. Loss: 0.182. F1-score: 0.414. Test loss: 0.171. Test F1-score: 0.368. Precision: 0.889. Recall: 0.232\n",
      "Epoch [19/50]. Loss: 0.179. F1-score: 0.453. Test loss: 0.166. Test F1-score: 0.384. Precision: 0.880. Recall: 0.246\n",
      "Epoch [20/50]. Loss: 0.177. F1-score: 0.472. Test loss: 0.166. Test F1-score: 0.403. Precision: 0.880. Recall: 0.261\n",
      "Epoch [21/50]. Loss: 0.168. F1-score: 0.502. Test loss: 0.163. Test F1-score: 0.414. Precision: 0.883. Recall: 0.270\n",
      "Epoch [22/50]. Loss: 0.166. F1-score: 0.515. Test loss: 0.161. Test F1-score: 0.428. Precision: 0.894. Recall: 0.281\n",
      "Epoch [23/50]. Loss: 0.163. F1-score: 0.529. Test loss: 0.160. Test F1-score: 0.438. Precision: 0.897. Recall: 0.290\n",
      "Epoch [24/50]. Loss: 0.160. F1-score: 0.548. Test loss: 0.161. Test F1-score: 0.440. Precision: 0.891. Recall: 0.292\n",
      "Epoch [25/50]. Loss: 0.160. F1-score: 0.550. Test loss: 0.164. Test F1-score: 0.455. Precision: 0.890. Recall: 0.306\n",
      "Epoch [26/50]. Loss: 0.152. F1-score: 0.567. Test loss: 0.161. Test F1-score: 0.473. Precision: 0.894. Recall: 0.321\n",
      "Epoch [27/50]. Loss: 0.149. F1-score: 0.576. Test loss: 0.161. Test F1-score: 0.479. Precision: 0.886. Recall: 0.328\n",
      "Epoch [28/50]. Loss: 0.148. F1-score: 0.589. Test loss: 0.162. Test F1-score: 0.485. Precision: 0.882. Recall: 0.335\n",
      "Epoch [29/50]. Loss: 0.146. F1-score: 0.592. Test loss: 0.160. Test F1-score: 0.490. Precision: 0.879. Recall: 0.339\n",
      "Epoch [30/50]. Loss: 0.147. F1-score: 0.598. Test loss: 0.164. Test F1-score: 0.493. Precision: 0.884. Recall: 0.342\n",
      "Epoch [31/50]. Loss: 0.143. F1-score: 0.609. Test loss: 0.164. Test F1-score: 0.502. Precision: 0.868. Recall: 0.353\n",
      "Epoch [32/50]. Loss: 0.138. F1-score: 0.630. Test loss: 0.166. Test F1-score: 0.516. Precision: 0.872. Recall: 0.366\n",
      "Epoch [33/50]. Loss: 0.137. F1-score: 0.630. Test loss: 0.162. Test F1-score: 0.525. Precision: 0.875. Recall: 0.375\n",
      "Epoch [34/50]. Loss: 0.135. F1-score: 0.639. Test loss: 0.169. Test F1-score: 0.516. Precision: 0.859. Recall: 0.368\n",
      "Epoch [35/50]. Loss: 0.135. F1-score: 0.640. Test loss: 0.166. Test F1-score: 0.512. Precision: 0.876. Recall: 0.362\n",
      "Epoch [36/50]. Loss: 0.131. F1-score: 0.643. Test loss: 0.167. Test F1-score: 0.514. Precision: 0.876. Recall: 0.364\n",
      "Epoch [37/50]. Loss: 0.130. F1-score: 0.652. Test loss: 0.169. Test F1-score: 0.517. Precision: 0.868. Recall: 0.368\n",
      "Epoch [38/50]. Loss: 0.128. F1-score: 0.667. Test loss: 0.168. Test F1-score: 0.520. Precision: 0.869. Recall: 0.371\n",
      "Epoch [39/50]. Loss: 0.127. F1-score: 0.668. Test loss: 0.168. Test F1-score: 0.517. Precision: 0.856. Recall: 0.371\n",
      "Epoch [40/50]. Loss: 0.128. F1-score: 0.671. Test loss: 0.172. Test F1-score: 0.513. Precision: 0.859. Recall: 0.366\n",
      "Epoch [41/50]. Loss: 0.124. F1-score: 0.683. Test loss: 0.172. Test F1-score: 0.542. Precision: 0.844. Recall: 0.400\n",
      "Epoch [42/50]. Loss: 0.123. F1-score: 0.692. Test loss: 0.174. Test F1-score: 0.534. Precision: 0.845. Recall: 0.391\n",
      "Epoch [43/50]. Loss: 0.122. F1-score: 0.683. Test loss: 0.176. Test F1-score: 0.532. Precision: 0.856. Recall: 0.386\n",
      "Epoch [44/50]. Loss: 0.122. F1-score: 0.682. Test loss: 0.178. Test F1-score: 0.523. Precision: 0.854. Recall: 0.377\n",
      "Epoch [45/50]. Loss: 0.120. F1-score: 0.692. Test loss: 0.177. Test F1-score: 0.532. Precision: 0.856. Recall: 0.386\n",
      "Epoch [46/50]. Loss: 0.119. F1-score: 0.698. Test loss: 0.173. Test F1-score: 0.542. Precision: 0.852. Recall: 0.397\n",
      "Epoch [47/50]. Loss: 0.120. F1-score: 0.701. Test loss: 0.181. Test F1-score: 0.546. Precision: 0.861. Recall: 0.400\n",
      "Epoch [48/50]. Loss: 0.116. F1-score: 0.703. Test loss: 0.183. Test F1-score: 0.548. Precision: 0.861. Recall: 0.402\n",
      "Epoch [49/50]. Loss: 0.115. F1-score: 0.705. Test loss: 0.181. Test F1-score: 0.550. Precision: 0.850. Recall: 0.406\n",
      "Epoch [50/50]. Loss: 0.115. F1-score: 0.708. Test loss: 0.182. Test F1-score: 0.561. Precision: 0.865. Recall: 0.415\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_2=train_nn(epochs=50, embedding_dim=128, hidden_size=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.758. F1-score: 0.153. Test loss: 0.242. Test F1-score: 0.090. Precision: 0.227. Recall: 0.056\n",
      "Epoch [2/50]. Loss: 0.386. F1-score: 0.135. Test loss: 0.216. Test F1-score: 0.109. Precision: 0.897. Recall: 0.058\n",
      "Epoch [3/50]. Loss: 0.339. F1-score: 0.185. Test loss: 0.206. Test F1-score: 0.136. Precision: 0.892. Recall: 0.074\n",
      "Epoch [4/50]. Loss: 0.307. F1-score: 0.215. Test loss: 0.197. Test F1-score: 0.173. Precision: 0.896. Recall: 0.096\n",
      "Epoch [5/50]. Loss: 0.288. F1-score: 0.223. Test loss: 0.189. Test F1-score: 0.181. Precision: 0.938. Recall: 0.100\n",
      "Epoch [6/50]. Loss: 0.267. F1-score: 0.244. Test loss: 0.187. Test F1-score: 0.185. Precision: 0.939. Recall: 0.103\n",
      "Epoch [7/50]. Loss: 0.256. F1-score: 0.255. Test loss: 0.184. Test F1-score: 0.202. Precision: 0.911. Recall: 0.114\n",
      "Epoch [8/50]. Loss: 0.242. F1-score: 0.277. Test loss: 0.178. Test F1-score: 0.237. Precision: 0.924. Recall: 0.136\n",
      "Epoch [9/50]. Loss: 0.231. F1-score: 0.301. Test loss: 0.177. Test F1-score: 0.267. Precision: 0.909. Recall: 0.156\n",
      "Epoch [10/50]. Loss: 0.225. F1-score: 0.327. Test loss: 0.173. Test F1-score: 0.287. Precision: 0.927. Recall: 0.170\n",
      "Epoch [11/50]. Loss: 0.214. F1-score: 0.362. Test loss: 0.171. Test F1-score: 0.314. Precision: 0.904. Recall: 0.190\n",
      "Epoch [12/50]. Loss: 0.211. F1-score: 0.401. Test loss: 0.169. Test F1-score: 0.344. Precision: 0.905. Recall: 0.212\n",
      "Epoch [13/50]. Loss: 0.204. F1-score: 0.428. Test loss: 0.167. Test F1-score: 0.376. Precision: 0.914. Recall: 0.237\n",
      "Epoch [14/50]. Loss: 0.195. F1-score: 0.454. Test loss: 0.162. Test F1-score: 0.403. Precision: 0.906. Recall: 0.259\n",
      "Epoch [15/50]. Loss: 0.192. F1-score: 0.480. Test loss: 0.164. Test F1-score: 0.415. Precision: 0.916. Recall: 0.268\n",
      "Epoch [16/50]. Loss: 0.186. F1-score: 0.500. Test loss: 0.159. Test F1-score: 0.425. Precision: 0.893. Recall: 0.279\n",
      "Epoch [17/50]. Loss: 0.183. F1-score: 0.518. Test loss: 0.158. Test F1-score: 0.431. Precision: 0.901. Recall: 0.283\n",
      "Epoch [18/50]. Loss: 0.176. F1-score: 0.537. Test loss: 0.159. Test F1-score: 0.452. Precision: 0.883. Recall: 0.304\n",
      "Epoch [19/50]. Loss: 0.175. F1-score: 0.566. Test loss: 0.154. Test F1-score: 0.467. Precision: 0.867. Recall: 0.319\n",
      "Epoch [20/50]. Loss: 0.171. F1-score: 0.570. Test loss: 0.156. Test F1-score: 0.468. Precision: 0.862. Recall: 0.321\n",
      "Epoch [21/50]. Loss: 0.167. F1-score: 0.588. Test loss: 0.156. Test F1-score: 0.485. Precision: 0.877. Recall: 0.335\n",
      "Epoch [22/50]. Loss: 0.163. F1-score: 0.590. Test loss: 0.153. Test F1-score: 0.498. Precision: 0.876. Recall: 0.348\n",
      "Epoch [23/50]. Loss: 0.157. F1-score: 0.602. Test loss: 0.154. Test F1-score: 0.492. Precision: 0.865. Recall: 0.344\n",
      "Epoch [24/50]. Loss: 0.154. F1-score: 0.618. Test loss: 0.152. Test F1-score: 0.513. Precision: 0.872. Recall: 0.364\n",
      "Epoch [25/50]. Loss: 0.150. F1-score: 0.623. Test loss: 0.154. Test F1-score: 0.512. Precision: 0.862. Recall: 0.364\n",
      "Epoch [26/50]. Loss: 0.148. F1-score: 0.632. Test loss: 0.153. Test F1-score: 0.519. Precision: 0.865. Recall: 0.371\n",
      "Epoch [27/50]. Loss: 0.147. F1-score: 0.633. Test loss: 0.153. Test F1-score: 0.520. Precision: 0.861. Recall: 0.373\n",
      "Epoch [28/50]. Loss: 0.144. F1-score: 0.633. Test loss: 0.154. Test F1-score: 0.531. Precision: 0.848. Recall: 0.386\n",
      "Epoch [29/50]. Loss: 0.141. F1-score: 0.656. Test loss: 0.153. Test F1-score: 0.531. Precision: 0.852. Recall: 0.386\n",
      "Epoch [30/50]. Loss: 0.142. F1-score: 0.662. Test loss: 0.157. Test F1-score: 0.528. Precision: 0.855. Recall: 0.382\n",
      "Epoch [31/50]. Loss: 0.136. F1-score: 0.665. Test loss: 0.157. Test F1-score: 0.528. Precision: 0.847. Recall: 0.384\n",
      "Epoch [32/50]. Loss: 0.135. F1-score: 0.683. Test loss: 0.157. Test F1-score: 0.539. Precision: 0.836. Recall: 0.397\n",
      "Epoch [33/50]. Loss: 0.130. F1-score: 0.684. Test loss: 0.163. Test F1-score: 0.534. Precision: 0.834. Recall: 0.393\n",
      "Epoch [34/50]. Loss: 0.129. F1-score: 0.696. Test loss: 0.155. Test F1-score: 0.544. Precision: 0.841. Recall: 0.402\n",
      "Epoch [35/50]. Loss: 0.128. F1-score: 0.693. Test loss: 0.156. Test F1-score: 0.551. Precision: 0.847. Recall: 0.408\n",
      "Epoch [36/50]. Loss: 0.127. F1-score: 0.705. Test loss: 0.161. Test F1-score: 0.547. Precision: 0.839. Recall: 0.406\n",
      "Epoch [37/50]. Loss: 0.122. F1-score: 0.716. Test loss: 0.160. Test F1-score: 0.553. Precision: 0.837. Recall: 0.413\n",
      "Epoch [38/50]. Loss: 0.122. F1-score: 0.723. Test loss: 0.158. Test F1-score: 0.551. Precision: 0.836. Recall: 0.411\n",
      "Epoch [39/50]. Loss: 0.126. F1-score: 0.707. Test loss: 0.160. Test F1-score: 0.545. Precision: 0.845. Recall: 0.402\n",
      "Epoch [40/50]. Loss: 0.120. F1-score: 0.718. Test loss: 0.162. Test F1-score: 0.555. Precision: 0.845. Recall: 0.413\n",
      "Epoch [41/50]. Loss: 0.117. F1-score: 0.731. Test loss: 0.160. Test F1-score: 0.562. Precision: 0.840. Recall: 0.422\n",
      "Epoch [42/50]. Loss: 0.118. F1-score: 0.730. Test loss: 0.161. Test F1-score: 0.569. Precision: 0.839. Recall: 0.431\n",
      "Epoch [43/50]. Loss: 0.118. F1-score: 0.731. Test loss: 0.169. Test F1-score: 0.552. Precision: 0.851. Recall: 0.408\n",
      "Epoch [44/50]. Loss: 0.118. F1-score: 0.729. Test loss: 0.168. Test F1-score: 0.562. Precision: 0.844. Recall: 0.422\n",
      "Epoch [45/50]. Loss: 0.114. F1-score: 0.741. Test loss: 0.167. Test F1-score: 0.574. Precision: 0.844. Recall: 0.435\n",
      "Epoch [46/50]. Loss: 0.114. F1-score: 0.747. Test loss: 0.169. Test F1-score: 0.566. Precision: 0.841. Recall: 0.426\n",
      "Epoch [47/50]. Loss: 0.106. F1-score: 0.758. Test loss: 0.169. Test F1-score: 0.568. Precision: 0.849. Recall: 0.426\n",
      "Epoch [48/50]. Loss: 0.110. F1-score: 0.752. Test loss: 0.175. Test F1-score: 0.562. Precision: 0.851. Recall: 0.420\n",
      "Epoch [49/50]. Loss: 0.109. F1-score: 0.752. Test loss: 0.177. Test F1-score: 0.555. Precision: 0.856. Recall: 0.411\n",
      "Epoch [50/50]. Loss: 0.109. F1-score: 0.755. Test loss: 0.182. Test F1-score: 0.561. Precision: 0.847. Recall: 0.420\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_3=train_nn(epochs=50, embedding_dim=256, hidden_size=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.517. F1-score: 0.155. Test loss: 0.224. Test F1-score: 0.136. Precision: 0.312. Recall: 0.087\n",
      "Epoch [2/50]. Loss: 0.315. F1-score: 0.216. Test loss: 0.200. Test F1-score: 0.168. Precision: 0.387. Recall: 0.107\n",
      "Epoch [3/50]. Loss: 0.277. F1-score: 0.256. Test loss: 0.191. Test F1-score: 0.197. Precision: 0.408. Recall: 0.129\n",
      "Epoch [4/50]. Loss: 0.259. F1-score: 0.295. Test loss: 0.187. Test F1-score: 0.222. Precision: 0.452. Recall: 0.147\n",
      "Epoch [5/50]. Loss: 0.243. F1-score: 0.324. Test loss: 0.176. Test F1-score: 0.287. Precision: 0.517. Recall: 0.199\n",
      "Epoch [6/50]. Loss: 0.226. F1-score: 0.390. Test loss: 0.173. Test F1-score: 0.357. Precision: 0.856. Recall: 0.225\n",
      "Epoch [7/50]. Loss: 0.215. F1-score: 0.425. Test loss: 0.170. Test F1-score: 0.368. Precision: 0.861. Recall: 0.234\n",
      "Epoch [8/50]. Loss: 0.208. F1-score: 0.468. Test loss: 0.168. Test F1-score: 0.378. Precision: 0.852. Recall: 0.243\n",
      "Epoch [9/50]. Loss: 0.202. F1-score: 0.471. Test loss: 0.165. Test F1-score: 0.413. Precision: 0.853. Recall: 0.272\n",
      "Epoch [10/50]. Loss: 0.194. F1-score: 0.512. Test loss: 0.162. Test F1-score: 0.426. Precision: 0.875. Recall: 0.281\n",
      "Epoch [11/50]. Loss: 0.184. F1-score: 0.527. Test loss: 0.159. Test F1-score: 0.444. Precision: 0.881. Recall: 0.297\n",
      "Epoch [12/50]. Loss: 0.177. F1-score: 0.548. Test loss: 0.160. Test F1-score: 0.462. Precision: 0.870. Recall: 0.315\n",
      "Epoch [13/50]. Loss: 0.174. F1-score: 0.576. Test loss: 0.157. Test F1-score: 0.477. Precision: 0.855. Recall: 0.330\n",
      "Epoch [14/50]. Loss: 0.167. F1-score: 0.601. Test loss: 0.155. Test F1-score: 0.500. Precision: 0.846. Recall: 0.355\n",
      "Epoch [15/50]. Loss: 0.162. F1-score: 0.623. Test loss: 0.159. Test F1-score: 0.518. Precision: 0.848. Recall: 0.373\n",
      "Epoch [16/50]. Loss: 0.160. F1-score: 0.629. Test loss: 0.155. Test F1-score: 0.515. Precision: 0.835. Recall: 0.373\n",
      "Epoch [17/50]. Loss: 0.154. F1-score: 0.649. Test loss: 0.157. Test F1-score: 0.512. Precision: 0.842. Recall: 0.368\n",
      "Epoch [18/50]. Loss: 0.151. F1-score: 0.667. Test loss: 0.159. Test F1-score: 0.528. Precision: 0.825. Recall: 0.388\n",
      "Epoch [19/50]. Loss: 0.146. F1-score: 0.675. Test loss: 0.159. Test F1-score: 0.533. Precision: 0.830. Recall: 0.393\n",
      "Epoch [20/50]. Loss: 0.143. F1-score: 0.692. Test loss: 0.155. Test F1-score: 0.541. Precision: 0.836. Recall: 0.400\n",
      "Epoch [21/50]. Loss: 0.143. F1-score: 0.705. Test loss: 0.159. Test F1-score: 0.536. Precision: 0.831. Recall: 0.395\n",
      "Epoch [22/50]. Loss: 0.141. F1-score: 0.709. Test loss: 0.164. Test F1-score: 0.521. Precision: 0.833. Recall: 0.379\n",
      "Epoch [23/50]. Loss: 0.138. F1-score: 0.711. Test loss: 0.159. Test F1-score: 0.546. Precision: 0.842. Recall: 0.404\n",
      "Epoch [24/50]. Loss: 0.135. F1-score: 0.726. Test loss: 0.163. Test F1-score: 0.553. Precision: 0.867. Recall: 0.406\n",
      "Epoch [25/50]. Loss: 0.130. F1-score: 0.726. Test loss: 0.163. Test F1-score: 0.554. Precision: 0.824. Recall: 0.417\n",
      "Epoch [26/50]. Loss: 0.132. F1-score: 0.731. Test loss: 0.165. Test F1-score: 0.538. Precision: 0.814. Recall: 0.402\n",
      "Epoch [27/50]. Loss: 0.128. F1-score: 0.732. Test loss: 0.167. Test F1-score: 0.547. Precision: 0.818. Recall: 0.411\n",
      "Epoch [28/50]. Loss: 0.125. F1-score: 0.745. Test loss: 0.167. Test F1-score: 0.543. Precision: 0.826. Recall: 0.404\n",
      "Epoch [29/50]. Loss: 0.123. F1-score: 0.745. Test loss: 0.164. Test F1-score: 0.553. Precision: 0.837. Recall: 0.413\n",
      "Epoch [30/50]. Loss: 0.121. F1-score: 0.760. Test loss: 0.172. Test F1-score: 0.558. Precision: 0.822. Recall: 0.422\n",
      "Epoch [31/50]. Loss: 0.120. F1-score: 0.765. Test loss: 0.172. Test F1-score: 0.548. Precision: 0.821. Recall: 0.411\n",
      "Epoch [32/50]. Loss: 0.119. F1-score: 0.766. Test loss: 0.173. Test F1-score: 0.556. Precision: 0.849. Recall: 0.413\n",
      "Epoch [33/50]. Loss: 0.115. F1-score: 0.768. Test loss: 0.174. Test F1-score: 0.556. Precision: 0.825. Recall: 0.420\n",
      "Epoch [34/50]. Loss: 0.113. F1-score: 0.774. Test loss: 0.177. Test F1-score: 0.562. Precision: 0.833. Recall: 0.424\n",
      "Epoch [35/50]. Loss: 0.113. F1-score: 0.785. Test loss: 0.173. Test F1-score: 0.565. Precision: 0.828. Recall: 0.429\n",
      "Epoch [36/50]. Loss: 0.112. F1-score: 0.775. Test loss: 0.175. Test F1-score: 0.568. Precision: 0.849. Recall: 0.426\n",
      "Epoch [37/50]. Loss: 0.113. F1-score: 0.779. Test loss: 0.182. Test F1-score: 0.558. Precision: 0.842. Recall: 0.417\n",
      "Epoch [38/50]. Loss: 0.110. F1-score: 0.784. Test loss: 0.181. Test F1-score: 0.568. Precision: 0.842. Recall: 0.429\n",
      "Epoch [39/50]. Loss: 0.110. F1-score: 0.787. Test loss: 0.182. Test F1-score: 0.568. Precision: 0.871. Recall: 0.422\n",
      "Epoch [40/50]. Loss: 0.105. F1-score: 0.788. Test loss: 0.184. Test F1-score: 0.565. Precision: 0.844. Recall: 0.424\n",
      "Epoch [41/50]. Loss: 0.103. F1-score: 0.799. Test loss: 0.184. Test F1-score: 0.570. Precision: 0.833. Recall: 0.433\n",
      "Epoch [42/50]. Loss: 0.104. F1-score: 0.799. Test loss: 0.194. Test F1-score: 0.571. Precision: 0.846. Recall: 0.431\n",
      "Epoch [43/50]. Loss: 0.104. F1-score: 0.795. Test loss: 0.189. Test F1-score: 0.560. Precision: 0.839. Recall: 0.420\n",
      "Epoch [44/50]. Loss: 0.101. F1-score: 0.803. Test loss: 0.194. Test F1-score: 0.568. Precision: 0.842. Recall: 0.429\n",
      "Epoch [45/50]. Loss: 0.102. F1-score: 0.809. Test loss: 0.195. Test F1-score: 0.566. Precision: 0.825. Recall: 0.431\n",
      "Epoch [46/50]. Loss: 0.101. F1-score: 0.812. Test loss: 0.195. Test F1-score: 0.577. Precision: 0.838. Recall: 0.440\n",
      "Epoch [47/50]. Loss: 0.101. F1-score: 0.817. Test loss: 0.198. Test F1-score: 0.563. Precision: 0.848. Recall: 0.422\n",
      "Epoch [48/50]. Loss: 0.098. F1-score: 0.814. Test loss: 0.206. Test F1-score: 0.585. Precision: 0.847. Recall: 0.446\n",
      "Epoch [49/50]. Loss: 0.099. F1-score: 0.819. Test loss: 0.201. Test F1-score: 0.583. Precision: 0.847. Recall: 0.444\n",
      "Epoch [50/50]. Loss: 0.095. F1-score: 0.827. Test loss: 0.203. Test F1-score: 0.588. Precision: 0.852. Recall: 0.449\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_4 = train_nn(epochs=50, embedding_dim=512, hidden_size=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.444. F1-score: 0.177. Test loss: 0.202. Test F1-score: 0.195. Precision: 0.441. Recall: 0.125\n",
      "Epoch [2/50]. Loss: 0.300. F1-score: 0.237. Test loss: 0.190. Test F1-score: 0.230. Precision: 0.908. Recall: 0.132\n",
      "Epoch [3/50]. Loss: 0.272. F1-score: 0.290. Test loss: 0.184. Test F1-score: 0.238. Precision: 0.938. Recall: 0.136\n",
      "Epoch [4/50]. Loss: 0.253. F1-score: 0.317. Test loss: 0.177. Test F1-score: 0.284. Precision: 0.926. Recall: 0.167\n",
      "Epoch [5/50]. Loss: 0.240. F1-score: 0.355. Test loss: 0.172. Test F1-score: 0.324. Precision: 0.926. Recall: 0.196\n",
      "Epoch [6/50]. Loss: 0.225. F1-score: 0.383. Test loss: 0.170. Test F1-score: 0.367. Precision: 0.912. Recall: 0.230\n",
      "Epoch [7/50]. Loss: 0.220. F1-score: 0.426. Test loss: 0.165. Test F1-score: 0.404. Precision: 0.943. Recall: 0.257\n",
      "Epoch [8/50]. Loss: 0.204. F1-score: 0.464. Test loss: 0.164. Test F1-score: 0.445. Precision: 0.910. Recall: 0.295\n",
      "Epoch [9/50]. Loss: 0.193. F1-score: 0.502. Test loss: 0.159. Test F1-score: 0.466. Precision: 0.915. Recall: 0.312\n",
      "Epoch [10/50]. Loss: 0.186. F1-score: 0.543. Test loss: 0.158. Test F1-score: 0.492. Precision: 0.894. Recall: 0.339\n",
      "Epoch [11/50]. Loss: 0.176. F1-score: 0.583. Test loss: 0.155. Test F1-score: 0.508. Precision: 0.866. Recall: 0.359\n",
      "Epoch [12/50]. Loss: 0.166. F1-score: 0.608. Test loss: 0.154. Test F1-score: 0.518. Precision: 0.873. Recall: 0.368\n",
      "Epoch [13/50]. Loss: 0.169. F1-score: 0.632. Test loss: 0.156. Test F1-score: 0.532. Precision: 0.856. Recall: 0.386\n",
      "Epoch [14/50]. Loss: 0.159. F1-score: 0.654. Test loss: 0.155. Test F1-score: 0.533. Precision: 0.849. Recall: 0.388\n",
      "Epoch [15/50]. Loss: 0.159. F1-score: 0.664. Test loss: 0.154. Test F1-score: 0.542. Precision: 0.876. Recall: 0.393\n",
      "Epoch [16/50]. Loss: 0.151. F1-score: 0.689. Test loss: 0.155. Test F1-score: 0.559. Precision: 0.864. Recall: 0.413\n",
      "Epoch [17/50]. Loss: 0.147. F1-score: 0.702. Test loss: 0.156. Test F1-score: 0.557. Precision: 0.864. Recall: 0.411\n",
      "Epoch [18/50]. Loss: 0.140. F1-score: 0.703. Test loss: 0.153. Test F1-score: 0.554. Precision: 0.859. Recall: 0.408\n",
      "Epoch [19/50]. Loss: 0.141. F1-score: 0.706. Test loss: 0.156. Test F1-score: 0.555. Precision: 0.856. Recall: 0.411\n",
      "Epoch [20/50]. Loss: 0.137. F1-score: 0.734. Test loss: 0.158. Test F1-score: 0.571. Precision: 0.857. Recall: 0.429\n",
      "Epoch [21/50]. Loss: 0.135. F1-score: 0.743. Test loss: 0.160. Test F1-score: 0.567. Precision: 0.874. Recall: 0.420\n",
      "Epoch [22/50]. Loss: 0.130. F1-score: 0.746. Test loss: 0.162. Test F1-score: 0.572. Precision: 0.833. Recall: 0.435\n",
      "Epoch [23/50]. Loss: 0.128. F1-score: 0.753. Test loss: 0.164. Test F1-score: 0.579. Precision: 0.863. Recall: 0.435\n",
      "Epoch [24/50]. Loss: 0.125. F1-score: 0.756. Test loss: 0.163. Test F1-score: 0.563. Precision: 0.848. Recall: 0.422\n",
      "Epoch [25/50]. Loss: 0.123. F1-score: 0.759. Test loss: 0.170. Test F1-score: 0.574. Precision: 0.862. Recall: 0.431\n",
      "Epoch [26/50]. Loss: 0.121. F1-score: 0.773. Test loss: 0.168. Test F1-score: 0.579. Precision: 0.845. Recall: 0.440\n",
      "Epoch [27/50]. Loss: 0.120. F1-score: 0.779. Test loss: 0.173. Test F1-score: 0.562. Precision: 0.858. Recall: 0.417\n",
      "Epoch [28/50]. Loss: 0.117. F1-score: 0.771. Test loss: 0.173. Test F1-score: 0.560. Precision: 0.833. Recall: 0.422\n",
      "Epoch [29/50]. Loss: 0.117. F1-score: 0.784. Test loss: 0.170. Test F1-score: 0.573. Precision: 0.847. Recall: 0.433\n",
      "Epoch [30/50]. Loss: 0.114. F1-score: 0.789. Test loss: 0.175. Test F1-score: 0.565. Precision: 0.848. Recall: 0.424\n",
      "Epoch [31/50]. Loss: 0.110. F1-score: 0.784. Test loss: 0.179. Test F1-score: 0.568. Precision: 0.853. Recall: 0.426\n",
      "Epoch [32/50]. Loss: 0.112. F1-score: 0.802. Test loss: 0.182. Test F1-score: 0.579. Precision: 0.863. Recall: 0.435\n",
      "Epoch [33/50]. Loss: 0.111. F1-score: 0.794. Test loss: 0.176. Test F1-score: 0.574. Precision: 0.851. Recall: 0.433\n",
      "Epoch [34/50]. Loss: 0.106. F1-score: 0.798. Test loss: 0.181. Test F1-score: 0.578. Precision: 0.852. Recall: 0.438\n",
      "Epoch [35/50]. Loss: 0.105. F1-score: 0.815. Test loss: 0.181. Test F1-score: 0.579. Precision: 0.839. Recall: 0.442\n",
      "Epoch [36/50]. Loss: 0.104. F1-score: 0.811. Test loss: 0.187. Test F1-score: 0.568. Precision: 0.853. Recall: 0.426\n",
      "Epoch [37/50]. Loss: 0.103. F1-score: 0.820. Test loss: 0.190. Test F1-score: 0.579. Precision: 0.863. Recall: 0.435\n",
      "Epoch [38/50]. Loss: 0.102. F1-score: 0.821. Test loss: 0.184. Test F1-score: 0.575. Precision: 0.831. Recall: 0.440\n",
      "Epoch [39/50]. Loss: 0.102. F1-score: 0.822. Test loss: 0.190. Test F1-score: 0.571. Precision: 0.846. Recall: 0.431\n",
      "Epoch [40/50]. Loss: 0.100. F1-score: 0.821. Test loss: 0.192. Test F1-score: 0.577. Precision: 0.855. Recall: 0.435\n",
      "Epoch [41/50]. Loss: 0.099. F1-score: 0.826. Test loss: 0.192. Test F1-score: 0.582. Precision: 0.837. Recall: 0.446\n",
      "Epoch [42/50]. Loss: 0.103. F1-score: 0.826. Test loss: 0.199. Test F1-score: 0.567. Precision: 0.838. Recall: 0.429\n",
      "Epoch [43/50]. Loss: 0.097. F1-score: 0.825. Test loss: 0.197. Test F1-score: 0.572. Precision: 0.833. Recall: 0.435\n",
      "Epoch [44/50]. Loss: 0.095. F1-score: 0.826. Test loss: 0.200. Test F1-score: 0.582. Precision: 0.867. Recall: 0.438\n",
      "Epoch [45/50]. Loss: 0.095. F1-score: 0.832. Test loss: 0.203. Test F1-score: 0.588. Precision: 0.862. Recall: 0.446\n",
      "Epoch [46/50]. Loss: 0.096. F1-score: 0.826. Test loss: 0.200. Test F1-score: 0.577. Precision: 0.848. Recall: 0.438\n",
      "Epoch [47/50]. Loss: 0.093. F1-score: 0.836. Test loss: 0.207. Test F1-score: 0.580. Precision: 0.843. Recall: 0.442\n",
      "Epoch [48/50]. Loss: 0.093. F1-score: 0.847. Test loss: 0.206. Test F1-score: 0.581. Precision: 0.850. Recall: 0.442\n",
      "Epoch [49/50]. Loss: 0.092. F1-score: 0.848. Test loss: 0.213. Test F1-score: 0.584. Precision: 0.861. Recall: 0.442\n",
      "Epoch [50/50]. Loss: 0.092. F1-score: 0.835. Test loss: 0.210. Test F1-score: 0.586. Precision: 0.835. Recall: 0.451\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_5 = train_nn(epochs=50, embedding_dim=1024, hidden_size=16, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.498. F1-score: 0.189. Test loss: 0.217. Test F1-score: 0.113. Precision: 0.964. Recall: 0.060\n",
      "Epoch [2/50]. Loss: 0.284. F1-score: 0.198. Test loss: 0.196. Test F1-score: 0.174. Precision: 0.935. Recall: 0.096\n",
      "Epoch [3/50]. Loss: 0.242. F1-score: 0.250. Test loss: 0.188. Test F1-score: 0.220. Precision: 0.903. Recall: 0.125\n",
      "Epoch [4/50]. Loss: 0.223. F1-score: 0.291. Test loss: 0.180. Test F1-score: 0.273. Precision: 0.900. Recall: 0.161\n",
      "Epoch [5/50]. Loss: 0.205. F1-score: 0.342. Test loss: 0.171. Test F1-score: 0.328. Precision: 0.891. Recall: 0.201\n",
      "Epoch [6/50]. Loss: 0.192. F1-score: 0.403. Test loss: 0.167. Test F1-score: 0.375. Precision: 0.898. Recall: 0.237\n",
      "Epoch [7/50]. Loss: 0.180. F1-score: 0.453. Test loss: 0.162. Test F1-score: 0.393. Precision: 0.890. Recall: 0.252\n",
      "Epoch [8/50]. Loss: 0.169. F1-score: 0.502. Test loss: 0.158. Test F1-score: 0.436. Precision: 0.872. Recall: 0.290\n",
      "Epoch [9/50]. Loss: 0.162. F1-score: 0.549. Test loss: 0.156. Test F1-score: 0.473. Precision: 0.850. Recall: 0.328\n",
      "Epoch [10/50]. Loss: 0.152. F1-score: 0.583. Test loss: 0.154. Test F1-score: 0.491. Precision: 0.842. Recall: 0.346\n",
      "Epoch [11/50]. Loss: 0.150. F1-score: 0.609. Test loss: 0.153. Test F1-score: 0.514. Precision: 0.816. Recall: 0.375\n",
      "Epoch [12/50]. Loss: 0.141. F1-score: 0.644. Test loss: 0.151. Test F1-score: 0.517. Precision: 0.799. Recall: 0.382\n",
      "Epoch [13/50]. Loss: 0.135. F1-score: 0.672. Test loss: 0.153. Test F1-score: 0.517. Precision: 0.803. Recall: 0.382\n",
      "Epoch [14/50]. Loss: 0.130. F1-score: 0.693. Test loss: 0.151. Test F1-score: 0.541. Precision: 0.809. Recall: 0.406\n",
      "Epoch [15/50]. Loss: 0.123. F1-score: 0.703. Test loss: 0.150. Test F1-score: 0.551. Precision: 0.800. Recall: 0.420\n",
      "Epoch [16/50]. Loss: 0.121. F1-score: 0.712. Test loss: 0.152. Test F1-score: 0.555. Precision: 0.802. Recall: 0.424\n",
      "Epoch [17/50]. Loss: 0.115. F1-score: 0.730. Test loss: 0.150. Test F1-score: 0.557. Precision: 0.797. Recall: 0.429\n",
      "Epoch [18/50]. Loss: 0.116. F1-score: 0.745. Test loss: 0.153. Test F1-score: 0.573. Precision: 0.800. Recall: 0.446\n",
      "Epoch [19/50]. Loss: 0.109. F1-score: 0.759. Test loss: 0.154. Test F1-score: 0.559. Precision: 0.803. Recall: 0.429\n",
      "Epoch [20/50]. Loss: 0.107. F1-score: 0.766. Test loss: 0.152. Test F1-score: 0.570. Precision: 0.774. Recall: 0.451\n",
      "Epoch [21/50]. Loss: 0.105. F1-score: 0.775. Test loss: 0.158. Test F1-score: 0.575. Precision: 0.787. Recall: 0.453\n",
      "Epoch [22/50]. Loss: 0.100. F1-score: 0.787. Test loss: 0.159. Test F1-score: 0.579. Precision: 0.780. Recall: 0.460\n",
      "Epoch [23/50]. Loss: 0.101. F1-score: 0.785. Test loss: 0.162. Test F1-score: 0.571. Precision: 0.780. Recall: 0.451\n",
      "Epoch [24/50]. Loss: 0.099. F1-score: 0.801. Test loss: 0.161. Test F1-score: 0.579. Precision: 0.780. Recall: 0.460\n",
      "Epoch [25/50]. Loss: 0.094. F1-score: 0.804. Test loss: 0.168. Test F1-score: 0.570. Precision: 0.782. Recall: 0.449\n",
      "Epoch [26/50]. Loss: 0.093. F1-score: 0.811. Test loss: 0.170. Test F1-score: 0.576. Precision: 0.772. Recall: 0.460\n",
      "Epoch [27/50]. Loss: 0.089. F1-score: 0.820. Test loss: 0.170. Test F1-score: 0.594. Precision: 0.758. Recall: 0.489\n",
      "Epoch [28/50]. Loss: 0.091. F1-score: 0.837. Test loss: 0.170. Test F1-score: 0.585. Precision: 0.765. Recall: 0.473\n",
      "Epoch [29/50]. Loss: 0.087. F1-score: 0.838. Test loss: 0.172. Test F1-score: 0.586. Precision: 0.763. Recall: 0.475\n",
      "Epoch [30/50]. Loss: 0.085. F1-score: 0.844. Test loss: 0.174. Test F1-score: 0.583. Precision: 0.753. Recall: 0.475\n",
      "Epoch [31/50]. Loss: 0.084. F1-score: 0.842. Test loss: 0.181. Test F1-score: 0.580. Precision: 0.761. Recall: 0.469\n",
      "Epoch [32/50]. Loss: 0.082. F1-score: 0.855. Test loss: 0.178. Test F1-score: 0.589. Precision: 0.755. Recall: 0.482\n",
      "Epoch [33/50]. Loss: 0.082. F1-score: 0.860. Test loss: 0.177. Test F1-score: 0.589. Precision: 0.747. Recall: 0.487\n",
      "Epoch [34/50]. Loss: 0.076. F1-score: 0.862. Test loss: 0.188. Test F1-score: 0.595. Precision: 0.760. Recall: 0.489\n",
      "Epoch [35/50]. Loss: 0.077. F1-score: 0.864. Test loss: 0.187. Test F1-score: 0.589. Precision: 0.767. Recall: 0.478\n",
      "Epoch [36/50]. Loss: 0.077. F1-score: 0.870. Test loss: 0.191. Test F1-score: 0.589. Precision: 0.784. Recall: 0.471\n",
      "Epoch [37/50]. Loss: 0.075. F1-score: 0.872. Test loss: 0.191. Test F1-score: 0.607. Precision: 0.794. Recall: 0.491\n",
      "Epoch [38/50]. Loss: 0.075. F1-score: 0.876. Test loss: 0.192. Test F1-score: 0.608. Precision: 0.771. Recall: 0.502\n",
      "Epoch [39/50]. Loss: 0.073. F1-score: 0.884. Test loss: 0.194. Test F1-score: 0.605. Precision: 0.765. Recall: 0.500\n",
      "Epoch [40/50]. Loss: 0.071. F1-score: 0.884. Test loss: 0.196. Test F1-score: 0.599. Precision: 0.769. Recall: 0.491\n",
      "Epoch [41/50]. Loss: 0.071. F1-score: 0.894. Test loss: 0.202. Test F1-score: 0.603. Precision: 0.759. Recall: 0.500\n",
      "Epoch [42/50]. Loss: 0.069. F1-score: 0.893. Test loss: 0.209. Test F1-score: 0.604. Precision: 0.778. Recall: 0.493\n",
      "Epoch [43/50]. Loss: 0.067. F1-score: 0.899. Test loss: 0.208. Test F1-score: 0.599. Precision: 0.786. Recall: 0.484\n",
      "Epoch [44/50]. Loss: 0.067. F1-score: 0.898. Test loss: 0.217. Test F1-score: 0.611. Precision: 0.779. Recall: 0.502\n",
      "Epoch [45/50]. Loss: 0.069. F1-score: 0.907. Test loss: 0.214. Test F1-score: 0.599. Precision: 0.758. Recall: 0.496\n",
      "Epoch [46/50]. Loss: 0.068. F1-score: 0.903. Test loss: 0.216. Test F1-score: 0.609. Precision: 0.778. Recall: 0.500\n",
      "Epoch [47/50]. Loss: 0.065. F1-score: 0.908. Test loss: 0.222. Test F1-score: 0.607. Precision: 0.777. Recall: 0.498\n",
      "Epoch [48/50]. Loss: 0.064. F1-score: 0.908. Test loss: 0.225. Test F1-score: 0.603. Precision: 0.771. Recall: 0.496\n",
      "Epoch [49/50]. Loss: 0.064. F1-score: 0.912. Test loss: 0.225. Test F1-score: 0.605. Precision: 0.776. Recall: 0.496\n",
      "Epoch [50/50]. Loss: 0.062. F1-score: 0.911. Test loss: 0.230. Test F1-score: 0.610. Precision: 0.780. Recall: 0.500\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_6 = train_nn(epochs=50, embedding_dim=512, hidden_size=32, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.295. F1-score: 0.193. Test loss: 0.191. Test F1-score: 0.252. Precision: 0.868. Recall: 0.147\n",
      "Epoch [2/50]. Loss: 0.204. F1-score: 0.346. Test loss: 0.170. Test F1-score: 0.375. Precision: 0.877. Recall: 0.239\n",
      "Epoch [3/50]. Loss: 0.179. F1-score: 0.455. Test loss: 0.161. Test F1-score: 0.408. Precision: 0.875. Recall: 0.266\n",
      "Epoch [4/50]. Loss: 0.165. F1-score: 0.520. Test loss: 0.153. Test F1-score: 0.482. Precision: 0.857. Recall: 0.335\n",
      "Epoch [5/50]. Loss: 0.147. F1-score: 0.597. Test loss: 0.146. Test F1-score: 0.523. Precision: 0.819. Recall: 0.384\n",
      "Epoch [6/50]. Loss: 0.139. F1-score: 0.636. Test loss: 0.145. Test F1-score: 0.538. Precision: 0.821. Recall: 0.400\n",
      "Epoch [7/50]. Loss: 0.128. F1-score: 0.675. Test loss: 0.148. Test F1-score: 0.556. Precision: 0.849. Recall: 0.413\n",
      "Epoch [8/50]. Loss: 0.124. F1-score: 0.704. Test loss: 0.146. Test F1-score: 0.573. Precision: 0.837. Recall: 0.435\n",
      "Epoch [9/50]. Loss: 0.114. F1-score: 0.727. Test loss: 0.145. Test F1-score: 0.576. Precision: 0.828. Recall: 0.442\n",
      "Epoch [10/50]. Loss: 0.110. F1-score: 0.748. Test loss: 0.148. Test F1-score: 0.570. Precision: 0.811. Recall: 0.440\n",
      "Epoch [11/50]. Loss: 0.106. F1-score: 0.756. Test loss: 0.147. Test F1-score: 0.577. Precision: 0.816. Recall: 0.446\n",
      "Epoch [12/50]. Loss: 0.102. F1-score: 0.778. Test loss: 0.150. Test F1-score: 0.587. Precision: 0.805. Recall: 0.462\n",
      "Epoch [13/50]. Loss: 0.092. F1-score: 0.794. Test loss: 0.151. Test F1-score: 0.590. Precision: 0.804. Recall: 0.467\n",
      "Epoch [14/50]. Loss: 0.093. F1-score: 0.811. Test loss: 0.148. Test F1-score: 0.613. Precision: 0.775. Recall: 0.507\n",
      "Epoch [15/50]. Loss: 0.088. F1-score: 0.823. Test loss: 0.157. Test F1-score: 0.599. Precision: 0.810. Recall: 0.475\n",
      "Epoch [16/50]. Loss: 0.084. F1-score: 0.841. Test loss: 0.157. Test F1-score: 0.596. Precision: 0.798. Recall: 0.475\n",
      "Epoch [17/50]. Loss: 0.082. F1-score: 0.847. Test loss: 0.163. Test F1-score: 0.616. Precision: 0.790. Recall: 0.504\n",
      "Epoch [18/50]. Loss: 0.081. F1-score: 0.858. Test loss: 0.164. Test F1-score: 0.602. Precision: 0.800. Recall: 0.482\n",
      "Epoch [19/50]. Loss: 0.078. F1-score: 0.861. Test loss: 0.166. Test F1-score: 0.611. Precision: 0.796. Recall: 0.496\n",
      "Epoch [20/50]. Loss: 0.075. F1-score: 0.867. Test loss: 0.168. Test F1-score: 0.612. Precision: 0.801. Recall: 0.496\n",
      "Epoch [21/50]. Loss: 0.072. F1-score: 0.880. Test loss: 0.173. Test F1-score: 0.606. Precision: 0.810. Recall: 0.484\n",
      "Epoch [22/50]. Loss: 0.072. F1-score: 0.885. Test loss: 0.175. Test F1-score: 0.601. Precision: 0.816. Recall: 0.475\n",
      "Epoch [23/50]. Loss: 0.070. F1-score: 0.896. Test loss: 0.177. Test F1-score: 0.609. Precision: 0.800. Recall: 0.491\n",
      "Epoch [24/50]. Loss: 0.067. F1-score: 0.893. Test loss: 0.182. Test F1-score: 0.610. Precision: 0.801. Recall: 0.493\n",
      "Epoch [25/50]. Loss: 0.065. F1-score: 0.908. Test loss: 0.189. Test F1-score: 0.606. Precision: 0.815. Recall: 0.482\n",
      "Epoch [26/50]. Loss: 0.065. F1-score: 0.904. Test loss: 0.188. Test F1-score: 0.605. Precision: 0.772. Recall: 0.498\n",
      "Epoch [27/50]. Loss: 0.065. F1-score: 0.911. Test loss: 0.196. Test F1-score: 0.612. Precision: 0.784. Recall: 0.502\n",
      "Epoch [28/50]. Loss: 0.064. F1-score: 0.914. Test loss: 0.197. Test F1-score: 0.609. Precision: 0.800. Recall: 0.491\n",
      "Epoch [29/50]. Loss: 0.060. F1-score: 0.925. Test loss: 0.197. Test F1-score: 0.622. Precision: 0.788. Recall: 0.513\n",
      "Epoch [30/50]. Loss: 0.057. F1-score: 0.921. Test loss: 0.205. Test F1-score: 0.612. Precision: 0.794. Recall: 0.498\n",
      "Epoch [31/50]. Loss: 0.055. F1-score: 0.926. Test loss: 0.204. Test F1-score: 0.617. Precision: 0.784. Recall: 0.509\n",
      "Epoch [32/50]. Loss: 0.055. F1-score: 0.924. Test loss: 0.210. Test F1-score: 0.626. Precision: 0.792. Recall: 0.518\n",
      "Epoch [33/50]. Loss: 0.056. F1-score: 0.929. Test loss: 0.215. Test F1-score: 0.612. Precision: 0.779. Recall: 0.504\n",
      "Epoch [34/50]. Loss: 0.054. F1-score: 0.932. Test loss: 0.214. Test F1-score: 0.619. Precision: 0.775. Recall: 0.516\n",
      "Epoch [35/50]. Loss: 0.053. F1-score: 0.940. Test loss: 0.221. Test F1-score: 0.623. Precision: 0.793. Recall: 0.513\n",
      "Epoch [36/50]. Loss: 0.049. F1-score: 0.942. Test loss: 0.224. Test F1-score: 0.626. Precision: 0.785. Recall: 0.520\n",
      "Epoch [37/50]. Loss: 0.051. F1-score: 0.946. Test loss: 0.222. Test F1-score: 0.626. Precision: 0.785. Recall: 0.520\n",
      "Epoch [38/50]. Loss: 0.048. F1-score: 0.940. Test loss: 0.231. Test F1-score: 0.619. Precision: 0.775. Recall: 0.516\n",
      "Epoch [39/50]. Loss: 0.049. F1-score: 0.948. Test loss: 0.229. Test F1-score: 0.616. Precision: 0.765. Recall: 0.516\n",
      "Epoch [40/50]. Loss: 0.048. F1-score: 0.947. Test loss: 0.238. Test F1-score: 0.615. Precision: 0.776. Recall: 0.509\n",
      "Epoch [41/50]. Loss: 0.045. F1-score: 0.948. Test loss: 0.249. Test F1-score: 0.612. Precision: 0.789. Recall: 0.500\n",
      "Epoch [42/50]. Loss: 0.047. F1-score: 0.947. Test loss: 0.247. Test F1-score: 0.606. Precision: 0.786. Recall: 0.493\n",
      "Epoch [43/50]. Loss: 0.046. F1-score: 0.953. Test loss: 0.247. Test F1-score: 0.612. Precision: 0.752. Recall: 0.516\n",
      "Epoch [44/50]. Loss: 0.045. F1-score: 0.956. Test loss: 0.255. Test F1-score: 0.608. Precision: 0.759. Recall: 0.507\n",
      "Epoch [45/50]. Loss: 0.043. F1-score: 0.954. Test loss: 0.265. Test F1-score: 0.615. Precision: 0.756. Recall: 0.518\n",
      "Epoch [46/50]. Loss: 0.043. F1-score: 0.955. Test loss: 0.270. Test F1-score: 0.615. Precision: 0.752. Recall: 0.520\n",
      "Epoch [47/50]. Loss: 0.043. F1-score: 0.953. Test loss: 0.273. Test F1-score: 0.609. Precision: 0.757. Recall: 0.509\n",
      "Epoch [48/50]. Loss: 0.041. F1-score: 0.958. Test loss: 0.274. Test F1-score: 0.618. Precision: 0.751. Recall: 0.525\n",
      "Epoch [49/50]. Loss: 0.043. F1-score: 0.957. Test loss: 0.288. Test F1-score: 0.626. Precision: 0.787. Recall: 0.520\n",
      "Epoch [50/50]. Loss: 0.042. F1-score: 0.954. Test loss: 0.285. Test F1-score: 0.612. Precision: 0.768. Recall: 0.509\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_7 = train_nn(epochs=50, embedding_dim=512, hidden_size=64, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.329. F1-score: 0.225. Test loss: 0.183. Test F1-score: 0.314. Precision: 0.544. Recall: 0.221\n",
      "Epoch [2/50]. Loss: 0.203. F1-score: 0.406. Test loss: 0.164. Test F1-score: 0.438. Precision: 0.831. Recall: 0.297\n",
      "Epoch [3/50]. Loss: 0.174. F1-score: 0.511. Test loss: 0.155. Test F1-score: 0.488. Precision: 0.829. Recall: 0.346\n",
      "Epoch [4/50]. Loss: 0.154. F1-score: 0.580. Test loss: 0.150. Test F1-score: 0.513. Precision: 0.859. Recall: 0.366\n",
      "Epoch [5/50]. Loss: 0.138. F1-score: 0.638. Test loss: 0.146. Test F1-score: 0.565. Precision: 0.828. Recall: 0.429\n",
      "Epoch [6/50]. Loss: 0.126. F1-score: 0.685. Test loss: 0.142. Test F1-score: 0.573. Precision: 0.821. Recall: 0.440\n",
      "Epoch [7/50]. Loss: 0.117. F1-score: 0.719. Test loss: 0.144. Test F1-score: 0.577. Precision: 0.816. Recall: 0.446\n",
      "Epoch [8/50]. Loss: 0.107. F1-score: 0.757. Test loss: 0.143. Test F1-score: 0.599. Precision: 0.836. Recall: 0.467\n",
      "Epoch [9/50]. Loss: 0.102. F1-score: 0.772. Test loss: 0.141. Test F1-score: 0.606. Precision: 0.821. Recall: 0.480\n",
      "Epoch [10/50]. Loss: 0.096. F1-score: 0.804. Test loss: 0.143. Test F1-score: 0.598. Precision: 0.848. Recall: 0.462\n",
      "Epoch [11/50]. Loss: 0.088. F1-score: 0.826. Test loss: 0.141. Test F1-score: 0.629. Precision: 0.795. Recall: 0.520\n",
      "Epoch [12/50]. Loss: 0.087. F1-score: 0.836. Test loss: 0.146. Test F1-score: 0.612. Precision: 0.807. Recall: 0.493\n",
      "Epoch [13/50]. Loss: 0.081. F1-score: 0.851. Test loss: 0.147. Test F1-score: 0.612. Precision: 0.789. Recall: 0.500\n",
      "Epoch [14/50]. Loss: 0.078. F1-score: 0.865. Test loss: 0.146. Test F1-score: 0.626. Precision: 0.792. Recall: 0.518\n",
      "Epoch [15/50]. Loss: 0.073. F1-score: 0.879. Test loss: 0.150. Test F1-score: 0.623. Precision: 0.788. Recall: 0.516\n",
      "Epoch [16/50]. Loss: 0.069. F1-score: 0.892. Test loss: 0.153. Test F1-score: 0.631. Precision: 0.796. Recall: 0.522\n",
      "Epoch [17/50]. Loss: 0.067. F1-score: 0.892. Test loss: 0.156. Test F1-score: 0.619. Precision: 0.796. Recall: 0.507\n",
      "Epoch [18/50]. Loss: 0.067. F1-score: 0.907. Test loss: 0.160. Test F1-score: 0.621. Precision: 0.790. Recall: 0.511\n",
      "Epoch [19/50]. Loss: 0.064. F1-score: 0.912. Test loss: 0.168. Test F1-score: 0.616. Precision: 0.790. Recall: 0.504\n",
      "Epoch [20/50]. Loss: 0.061. F1-score: 0.912. Test loss: 0.166. Test F1-score: 0.608. Precision: 0.766. Recall: 0.504\n",
      "Epoch [21/50]. Loss: 0.058. F1-score: 0.921. Test loss: 0.174. Test F1-score: 0.625. Precision: 0.804. Recall: 0.511\n",
      "Epoch [22/50]. Loss: 0.057. F1-score: 0.924. Test loss: 0.173. Test F1-score: 0.634. Precision: 0.795. Recall: 0.527\n",
      "Epoch [23/50]. Loss: 0.057. F1-score: 0.935. Test loss: 0.172. Test F1-score: 0.636. Precision: 0.782. Recall: 0.536\n",
      "Epoch [24/50]. Loss: 0.053. F1-score: 0.939. Test loss: 0.178. Test F1-score: 0.642. Precision: 0.807. Recall: 0.533\n",
      "Epoch [25/50]. Loss: 0.052. F1-score: 0.942. Test loss: 0.186. Test F1-score: 0.624. Precision: 0.791. Recall: 0.516\n",
      "Epoch [26/50]. Loss: 0.048. F1-score: 0.943. Test loss: 0.188. Test F1-score: 0.622. Precision: 0.788. Recall: 0.513\n",
      "Epoch [27/50]. Loss: 0.047. F1-score: 0.947. Test loss: 0.191. Test F1-score: 0.615. Precision: 0.787. Recall: 0.504\n",
      "Epoch [28/50]. Loss: 0.045. F1-score: 0.947. Test loss: 0.194. Test F1-score: 0.628. Precision: 0.755. Recall: 0.538\n",
      "Epoch [29/50]. Loss: 0.047. F1-score: 0.952. Test loss: 0.191. Test F1-score: 0.620. Precision: 0.748. Recall: 0.529\n",
      "Epoch [30/50]. Loss: 0.044. F1-score: 0.951. Test loss: 0.196. Test F1-score: 0.627. Precision: 0.765. Recall: 0.531\n",
      "Epoch [31/50]. Loss: 0.044. F1-score: 0.955. Test loss: 0.198. Test F1-score: 0.623. Precision: 0.768. Recall: 0.525\n",
      "Epoch [32/50]. Loss: 0.041. F1-score: 0.960. Test loss: 0.199. Test F1-score: 0.628. Precision: 0.749. Recall: 0.540\n",
      "Epoch [33/50]. Loss: 0.040. F1-score: 0.961. Test loss: 0.209. Test F1-score: 0.627. Precision: 0.769. Recall: 0.529\n",
      "Epoch [34/50]. Loss: 0.039. F1-score: 0.961. Test loss: 0.211. Test F1-score: 0.635. Precision: 0.756. Recall: 0.547\n",
      "Epoch [35/50]. Loss: 0.042. F1-score: 0.959. Test loss: 0.216. Test F1-score: 0.626. Precision: 0.776. Recall: 0.525\n",
      "Epoch [36/50]. Loss: 0.040. F1-score: 0.963. Test loss: 0.221. Test F1-score: 0.624. Precision: 0.760. Recall: 0.529\n",
      "Epoch [37/50]. Loss: 0.039. F1-score: 0.966. Test loss: 0.229. Test F1-score: 0.621. Precision: 0.772. Recall: 0.520\n",
      "Epoch [38/50]. Loss: 0.036. F1-score: 0.966. Test loss: 0.235. Test F1-score: 0.632. Precision: 0.772. Recall: 0.536\n",
      "Epoch [39/50]. Loss: 0.036. F1-score: 0.967. Test loss: 0.227. Test F1-score: 0.634. Precision: 0.768. Recall: 0.540\n",
      "Epoch [40/50]. Loss: 0.036. F1-score: 0.967. Test loss: 0.228. Test F1-score: 0.636. Precision: 0.765. Recall: 0.545\n",
      "Epoch [41/50]. Loss: 0.034. F1-score: 0.969. Test loss: 0.238. Test F1-score: 0.621. Precision: 0.765. Recall: 0.522\n",
      "Epoch [42/50]. Loss: 0.034. F1-score: 0.968. Test loss: 0.256. Test F1-score: 0.629. Precision: 0.754. Recall: 0.540\n",
      "Epoch [43/50]. Loss: 0.035. F1-score: 0.971. Test loss: 0.255. Test F1-score: 0.632. Precision: 0.769. Recall: 0.536\n",
      "Epoch [44/50]. Loss: 0.033. F1-score: 0.970. Test loss: 0.263. Test F1-score: 0.636. Precision: 0.757. Recall: 0.549\n",
      "Epoch [45/50]. Loss: 0.033. F1-score: 0.971. Test loss: 0.267. Test F1-score: 0.621. Precision: 0.761. Recall: 0.525\n",
      "Epoch [46/50]. Loss: 0.033. F1-score: 0.971. Test loss: 0.269. Test F1-score: 0.626. Precision: 0.767. Recall: 0.529\n",
      "Epoch [47/50]. Loss: 0.031. F1-score: 0.972. Test loss: 0.266. Test F1-score: 0.629. Precision: 0.777. Recall: 0.529\n",
      "Epoch [48/50]. Loss: 0.031. F1-score: 0.972. Test loss: 0.265. Test F1-score: 0.626. Precision: 0.763. Recall: 0.531\n",
      "Epoch [49/50]. Loss: 0.028. F1-score: 0.972. Test loss: 0.271. Test F1-score: 0.633. Precision: 0.763. Recall: 0.540\n",
      "Epoch [50/50]. Loss: 0.027. F1-score: 0.971. Test loss: 0.282. Test F1-score: 0.630. Precision: 0.752. Recall: 0.542\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_8 = train_nn(epochs=50, embedding_dim=512, hidden_size=128, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.379. F1-score: 0.277. Test loss: 0.179. Test F1-score: 0.421. Precision: 0.588. Recall: 0.328\n",
      "Epoch [2/50]. Loss: 0.200. F1-score: 0.449. Test loss: 0.165. Test F1-score: 0.436. Precision: 0.616. Recall: 0.337\n",
      "Epoch [3/50]. Loss: 0.161. F1-score: 0.595. Test loss: 0.152. Test F1-score: 0.536. Precision: 0.831. Recall: 0.395\n",
      "Epoch [4/50]. Loss: 0.137. F1-score: 0.649. Test loss: 0.144. Test F1-score: 0.562. Precision: 0.823. Recall: 0.426\n",
      "Epoch [5/50]. Loss: 0.124. F1-score: 0.706. Test loss: 0.142. Test F1-score: 0.563. Precision: 0.848. Recall: 0.422\n",
      "Epoch [6/50]. Loss: 0.113. F1-score: 0.749. Test loss: 0.140. Test F1-score: 0.603. Precision: 0.853. Recall: 0.467\n",
      "Epoch [7/50]. Loss: 0.103. F1-score: 0.772. Test loss: 0.141. Test F1-score: 0.598. Precision: 0.855. Recall: 0.460\n",
      "Epoch [8/50]. Loss: 0.094. F1-score: 0.813. Test loss: 0.139. Test F1-score: 0.612. Precision: 0.826. Recall: 0.487\n",
      "Epoch [9/50]. Loss: 0.087. F1-score: 0.828. Test loss: 0.145. Test F1-score: 0.601. Precision: 0.837. Recall: 0.469\n",
      "Epoch [10/50]. Loss: 0.081. F1-score: 0.849. Test loss: 0.142. Test F1-score: 0.619. Precision: 0.817. Recall: 0.498\n",
      "Epoch [11/50]. Loss: 0.077. F1-score: 0.875. Test loss: 0.145. Test F1-score: 0.618. Precision: 0.814. Recall: 0.498\n",
      "Epoch [12/50]. Loss: 0.071. F1-score: 0.892. Test loss: 0.144. Test F1-score: 0.629. Precision: 0.791. Recall: 0.522\n",
      "Epoch [13/50]. Loss: 0.069. F1-score: 0.900. Test loss: 0.148. Test F1-score: 0.637. Precision: 0.823. Recall: 0.520\n",
      "Epoch [14/50]. Loss: 0.063. F1-score: 0.913. Test loss: 0.146. Test F1-score: 0.635. Precision: 0.820. Recall: 0.518\n",
      "Epoch [15/50]. Loss: 0.059. F1-score: 0.922. Test loss: 0.153. Test F1-score: 0.637. Precision: 0.823. Recall: 0.520\n",
      "Epoch [16/50]. Loss: 0.057. F1-score: 0.925. Test loss: 0.158. Test F1-score: 0.634. Precision: 0.807. Recall: 0.522\n",
      "Epoch [17/50]. Loss: 0.053. F1-score: 0.935. Test loss: 0.162. Test F1-score: 0.631. Precision: 0.787. Recall: 0.527\n",
      "Epoch [18/50]. Loss: 0.053. F1-score: 0.943. Test loss: 0.161. Test F1-score: 0.638. Precision: 0.803. Recall: 0.529\n",
      "Epoch [19/50]. Loss: 0.051. F1-score: 0.944. Test loss: 0.166. Test F1-score: 0.640. Precision: 0.799. Recall: 0.533\n",
      "Epoch [20/50]. Loss: 0.046. F1-score: 0.950. Test loss: 0.172. Test F1-score: 0.634. Precision: 0.795. Recall: 0.527\n",
      "Epoch [21/50]. Loss: 0.043. F1-score: 0.953. Test loss: 0.175. Test F1-score: 0.624. Precision: 0.806. Recall: 0.509\n",
      "Epoch [22/50]. Loss: 0.047. F1-score: 0.955. Test loss: 0.176. Test F1-score: 0.637. Precision: 0.801. Recall: 0.529\n",
      "Epoch [23/50]. Loss: 0.043. F1-score: 0.957. Test loss: 0.183. Test F1-score: 0.639. Precision: 0.811. Recall: 0.527\n",
      "Epoch [24/50]. Loss: 0.042. F1-score: 0.963. Test loss: 0.186. Test F1-score: 0.640. Precision: 0.817. Recall: 0.527\n",
      "Epoch [25/50]. Loss: 0.039. F1-score: 0.961. Test loss: 0.189. Test F1-score: 0.642. Precision: 0.795. Recall: 0.538\n",
      "Epoch [26/50]. Loss: 0.039. F1-score: 0.964. Test loss: 0.189. Test F1-score: 0.651. Precision: 0.818. Recall: 0.540\n",
      "Epoch [27/50]. Loss: 0.038. F1-score: 0.968. Test loss: 0.187. Test F1-score: 0.641. Precision: 0.797. Recall: 0.536\n",
      "Epoch [28/50]. Loss: 0.035. F1-score: 0.970. Test loss: 0.192. Test F1-score: 0.639. Precision: 0.779. Recall: 0.542\n",
      "Epoch [29/50]. Loss: 0.034. F1-score: 0.970. Test loss: 0.201. Test F1-score: 0.640. Precision: 0.781. Recall: 0.542\n",
      "Epoch [30/50]. Loss: 0.035. F1-score: 0.971. Test loss: 0.202. Test F1-score: 0.648. Precision: 0.787. Recall: 0.551\n",
      "Epoch [31/50]. Loss: 0.034. F1-score: 0.970. Test loss: 0.203. Test F1-score: 0.644. Precision: 0.801. Recall: 0.538\n",
      "Epoch [32/50]. Loss: 0.033. F1-score: 0.971. Test loss: 0.211. Test F1-score: 0.645. Precision: 0.794. Recall: 0.542\n",
      "Epoch [33/50]. Loss: 0.031. F1-score: 0.972. Test loss: 0.212. Test F1-score: 0.646. Precision: 0.804. Recall: 0.540\n",
      "Epoch [34/50]. Loss: 0.032. F1-score: 0.973. Test loss: 0.213. Test F1-score: 0.648. Precision: 0.814. Recall: 0.538\n",
      "Epoch [35/50]. Loss: 0.031. F1-score: 0.973. Test loss: 0.215. Test F1-score: 0.646. Precision: 0.809. Recall: 0.538\n",
      "Epoch [36/50]. Loss: 0.031. F1-score: 0.973. Test loss: 0.215. Test F1-score: 0.641. Precision: 0.784. Recall: 0.542\n",
      "Epoch [37/50]. Loss: 0.030. F1-score: 0.975. Test loss: 0.226. Test F1-score: 0.632. Precision: 0.806. Recall: 0.520\n",
      "Epoch [38/50]. Loss: 0.031. F1-score: 0.975. Test loss: 0.216. Test F1-score: 0.646. Precision: 0.767. Recall: 0.558\n",
      "Epoch [39/50]. Loss: 0.028. F1-score: 0.975. Test loss: 0.228. Test F1-score: 0.636. Precision: 0.803. Recall: 0.527\n",
      "Epoch [40/50]. Loss: 0.028. F1-score: 0.974. Test loss: 0.234. Test F1-score: 0.647. Precision: 0.761. Recall: 0.562\n",
      "Epoch [41/50]. Loss: 0.025. F1-score: 0.976. Test loss: 0.243. Test F1-score: 0.649. Precision: 0.794. Recall: 0.549\n",
      "Epoch [42/50]. Loss: 0.024. F1-score: 0.977. Test loss: 0.246. Test F1-score: 0.646. Precision: 0.767. Recall: 0.558\n",
      "Epoch [43/50]. Loss: 0.024. F1-score: 0.978. Test loss: 0.258. Test F1-score: 0.635. Precision: 0.784. Recall: 0.533\n",
      "Epoch [44/50]. Loss: 0.026. F1-score: 0.977. Test loss: 0.254. Test F1-score: 0.637. Precision: 0.767. Recall: 0.545\n",
      "Epoch [45/50]. Loss: 0.024. F1-score: 0.978. Test loss: 0.255. Test F1-score: 0.642. Precision: 0.800. Recall: 0.536\n",
      "Epoch [46/50]. Loss: 0.023. F1-score: 0.977. Test loss: 0.253. Test F1-score: 0.648. Precision: 0.764. Recall: 0.562\n",
      "Epoch [47/50]. Loss: 0.025. F1-score: 0.980. Test loss: 0.259. Test F1-score: 0.642. Precision: 0.774. Recall: 0.549\n",
      "Epoch [48/50]. Loss: 0.023. F1-score: 0.980. Test loss: 0.278. Test F1-score: 0.646. Precision: 0.771. Recall: 0.556\n",
      "Epoch [49/50]. Loss: 0.023. F1-score: 0.978. Test loss: 0.265. Test F1-score: 0.646. Precision: 0.783. Recall: 0.549\n",
      "Epoch [50/50]. Loss: 0.024. F1-score: 0.979. Test loss: 0.276. Test F1-score: 0.641. Precision: 0.780. Recall: 0.545\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_9 = train_nn(epochs=50, embedding_dim=512, hidden_size=256, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]. Loss: 0.378. F1-score: 0.281. Test loss: 0.166. Test F1-score: 0.419. Precision: 0.885. Recall: 0.275\n",
      "Epoch [2/50]. Loss: 0.151. F1-score: 0.558. Test loss: 0.148. Test F1-score: 0.561. Precision: 0.781. Recall: 0.438\n",
      "Epoch [3/50]. Loss: 0.124. F1-score: 0.688. Test loss: 0.149. Test F1-score: 0.583. Precision: 0.795. Recall: 0.460\n",
      "Epoch [4/50]. Loss: 0.104. F1-score: 0.741. Test loss: 0.150. Test F1-score: 0.604. Precision: 0.762. Recall: 0.500\n",
      "Epoch [5/50]. Loss: 0.096. F1-score: 0.772. Test loss: 0.156. Test F1-score: 0.598. Precision: 0.753. Recall: 0.496\n",
      "Epoch [6/50]. Loss: 0.088. F1-score: 0.816. Test loss: 0.167. Test F1-score: 0.611. Precision: 0.700. Recall: 0.542\n",
      "Epoch [7/50]. Loss: 0.077. F1-score: 0.836. Test loss: 0.173. Test F1-score: 0.629. Precision: 0.702. Recall: 0.569\n",
      "Epoch [8/50]. Loss: 0.069. F1-score: 0.865. Test loss: 0.196. Test F1-score: 0.608. Precision: 0.771. Recall: 0.502\n",
      "Epoch [9/50]. Loss: 0.063. F1-score: 0.884. Test loss: 0.202. Test F1-score: 0.620. Precision: 0.773. Recall: 0.518\n",
      "Epoch [10/50]. Loss: 0.057. F1-score: 0.903. Test loss: 0.218. Test F1-score: 0.623. Precision: 0.725. Recall: 0.547\n",
      "Epoch [11/50]. Loss: 0.053. F1-score: 0.922. Test loss: 0.227. Test F1-score: 0.625. Precision: 0.701. Recall: 0.565\n",
      "Epoch [12/50]. Loss: 0.047. F1-score: 0.936. Test loss: 0.251. Test F1-score: 0.624. Precision: 0.715. Recall: 0.554\n",
      "Epoch [13/50]. Loss: 0.044. F1-score: 0.944. Test loss: 0.258. Test F1-score: 0.615. Precision: 0.688. Recall: 0.556\n",
      "Epoch [14/50]. Loss: 0.042. F1-score: 0.949. Test loss: 0.269. Test F1-score: 0.629. Precision: 0.709. Recall: 0.565\n",
      "Epoch [15/50]. Loss: 0.039. F1-score: 0.956. Test loss: 0.295. Test F1-score: 0.613. Precision: 0.726. Recall: 0.531\n",
      "Epoch [16/50]. Loss: 0.037. F1-score: 0.953. Test loss: 0.330. Test F1-score: 0.617. Precision: 0.708. Recall: 0.547\n",
      "Epoch [17/50]. Loss: 0.034. F1-score: 0.959. Test loss: 0.324. Test F1-score: 0.614. Precision: 0.712. Recall: 0.540\n",
      "Epoch [18/50]. Loss: 0.032. F1-score: 0.963. Test loss: 0.336. Test F1-score: 0.617. Precision: 0.711. Recall: 0.545\n",
      "Epoch [19/50]. Loss: 0.031. F1-score: 0.966. Test loss: 0.347. Test F1-score: 0.622. Precision: 0.691. Recall: 0.565\n",
      "Epoch [20/50]. Loss: 0.030. F1-score: 0.968. Test loss: 0.358. Test F1-score: 0.618. Precision: 0.707. Recall: 0.549\n",
      "Epoch [21/50]. Loss: 0.027. F1-score: 0.970. Test loss: 0.361. Test F1-score: 0.616. Precision: 0.738. Recall: 0.529\n",
      "Epoch [22/50]. Loss: 0.026. F1-score: 0.972. Test loss: 0.377. Test F1-score: 0.620. Precision: 0.711. Recall: 0.549\n",
      "Epoch [23/50]. Loss: 0.027. F1-score: 0.971. Test loss: 0.405. Test F1-score: 0.644. Precision: 0.752. Recall: 0.562\n",
      "Epoch [24/50]. Loss: 0.029. F1-score: 0.973. Test loss: 0.398. Test F1-score: 0.633. Precision: 0.709. Recall: 0.571\n",
      "Epoch [25/50]. Loss: 0.026. F1-score: 0.973. Test loss: 0.384. Test F1-score: 0.631. Precision: 0.715. Recall: 0.565\n",
      "Epoch [26/50]. Loss: 0.024. F1-score: 0.974. Test loss: 0.412. Test F1-score: 0.627. Precision: 0.739. Recall: 0.545\n",
      "Epoch [27/50]. Loss: 0.023. F1-score: 0.975. Test loss: 0.466. Test F1-score: 0.624. Precision: 0.684. Recall: 0.574\n",
      "Epoch [28/50]. Loss: 0.022. F1-score: 0.976. Test loss: 0.445. Test F1-score: 0.625. Precision: 0.713. Recall: 0.556\n",
      "Epoch [29/50]. Loss: 0.022. F1-score: 0.977. Test loss: 0.479. Test F1-score: 0.611. Precision: 0.671. Recall: 0.560\n",
      "Epoch [30/50]. Loss: 0.021. F1-score: 0.976. Test loss: 0.485. Test F1-score: 0.616. Precision: 0.676. Recall: 0.565\n",
      "Epoch [31/50]. Loss: 0.025. F1-score: 0.977. Test loss: 0.475. Test F1-score: 0.610. Precision: 0.690. Recall: 0.547\n",
      "Epoch [32/50]. Loss: 0.020. F1-score: 0.978. Test loss: 0.529. Test F1-score: 0.608. Precision: 0.689. Recall: 0.545\n",
      "Epoch [33/50]. Loss: 0.020. F1-score: 0.978. Test loss: 0.519. Test F1-score: 0.618. Precision: 0.696. Recall: 0.556\n",
      "Epoch [34/50]. Loss: 0.020. F1-score: 0.979. Test loss: 0.502. Test F1-score: 0.624. Precision: 0.708. Recall: 0.558\n",
      "Epoch [35/50]. Loss: 0.020. F1-score: 0.977. Test loss: 0.526. Test F1-score: 0.621. Precision: 0.695. Recall: 0.560\n",
      "Epoch [36/50]. Loss: 0.020. F1-score: 0.977. Test loss: 0.541. Test F1-score: 0.603. Precision: 0.673. Recall: 0.547\n",
      "Epoch [37/50]. Loss: 0.018. F1-score: 0.980. Test loss: 0.555. Test F1-score: 0.614. Precision: 0.694. Recall: 0.551\n",
      "Epoch [38/50]. Loss: 0.018. F1-score: 0.979. Test loss: 0.565. Test F1-score: 0.614. Precision: 0.676. Recall: 0.562\n",
      "Epoch [39/50]. Loss: 0.016. F1-score: 0.978. Test loss: 0.593. Test F1-score: 0.614. Precision: 0.697. Recall: 0.549\n",
      "Epoch [40/50]. Loss: 0.018. F1-score: 0.980. Test loss: 0.565. Test F1-score: 0.617. Precision: 0.658. Recall: 0.580\n",
      "Epoch [41/50]. Loss: 0.018. F1-score: 0.977. Test loss: 0.591. Test F1-score: 0.623. Precision: 0.707. Recall: 0.556\n",
      "Epoch [42/50]. Loss: 0.019. F1-score: 0.979. Test loss: 0.588. Test F1-score: 0.616. Precision: 0.678. Recall: 0.565\n",
      "Epoch [43/50]. Loss: 0.016. F1-score: 0.979. Test loss: 0.656. Test F1-score: 0.625. Precision: 0.729. Recall: 0.547\n",
      "Epoch [44/50]. Loss: 0.017. F1-score: 0.978. Test loss: 0.643. Test F1-score: 0.608. Precision: 0.699. Recall: 0.538\n",
      "Epoch [45/50]. Loss: 0.016. F1-score: 0.978. Test loss: 0.606. Test F1-score: 0.621. Precision: 0.684. Recall: 0.569\n",
      "Epoch [46/50]. Loss: 0.016. F1-score: 0.980. Test loss: 0.621. Test F1-score: 0.624. Precision: 0.691. Recall: 0.569\n",
      "Epoch [47/50]. Loss: 0.017. F1-score: 0.980. Test loss: 0.647. Test F1-score: 0.624. Precision: 0.674. Recall: 0.580\n",
      "Epoch [48/50]. Loss: 0.018. F1-score: 0.979. Test loss: 0.691. Test F1-score: 0.619. Precision: 0.678. Recall: 0.569\n",
      "Epoch [49/50]. Loss: 0.016. F1-score: 0.980. Test loss: 0.639. Test F1-score: 0.612. Precision: 0.683. Recall: 0.554\n",
      "Epoch [50/50]. Loss: 0.017. F1-score: 0.980. Test loss: 0.708. Test F1-score: 0.627. Precision: 0.682. Recall: 0.580\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "net_10 = train_nn(epochs=50, embedding_dim=512, hidden_size=256, lr=0.01, return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: Epoch>8 приводит к переобучению и снижению метрики F1-score на тестовых данных, embedding_dim=512 при прочих равных условиях показало наилучшую  метрику F1-score, hidden_size=256 при соблюдении соотношении 1:2 с embedding_dim также улучшает показатель F1-score (сильна корреляция с embedding_dim), lr=0.01 довел точность до максимума (lr=0.1 просто \"сломал\" работу нейронной сети). Повысить точность нейронной сети помогла тонкая настройка основных параметров Epoch, embedding_dim и hidden_size. Лучший результат Test F1-score=0.629 на 7 эпохе с параметрами (embedding_dim=512, hidden_size=256, lr=0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "web_6.ipynb",
   "provenance": [
    {
     "file_id": "1WgwQ-cYhMmkeXpxrr3sLU4DJEcbcikkH",
     "timestamp": 1657278544369
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
