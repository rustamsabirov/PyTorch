{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 7. Рекурентные сети для обработки последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "1. Попробуйте обучить нейронную сеть GRU/LSTM для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
    "\n",
    "2. Опишите, какой результат вы получили? Что помогло вам улучшить ее точность?\n",
    "\n",
    "У кого нет возможности работать через каггл (нет верификации), то можете данные взять по ссылке: https://drive.google.com/file/d/1czQcI0Zgvgo6DjW1-yTFUhL8_XVsF6vi/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context\n",
    "# The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate \n",
    "# speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from \n",
    "# other tweets.\n",
    "# Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' \n",
    "# denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.\n",
    "\n",
    "# Контекст\n",
    "# Цель этой задачи - обнаружить ненавистнические высказывания в твитах. Для простоты мы говорим, что твит содержит \n",
    "# ненавистнические высказывания, если с ним связаны расистские или сексистские настроения. Итак, задача состоит в том, чтобы \n",
    "# отличить расистские или сексистские твиты от других твитов.\n",
    "# Формально, учитывая обучающую выборку твитов и меток, где метка \"1\" означает, что твит является расистским/сексистским, \n",
    "# а метка \"0\" означает, что твит не является расистским /сексистским, ваша цель - предсказать метки в тестовом наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '\\PyTorch-7'\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\",index_col=0)\n",
    "df_test = pd.read_csv(\"test.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>0</td>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>0</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "id                                                             \n",
       "1          0   @user when a father is dysfunctional and is s...\n",
       "2          0  @user @user thanks for #lyft credit i can't us...\n",
       "3          0                                bihday your majesty\n",
       "4          0  #model   i love u take with u all the time in ...\n",
       "5          0             factsguide: society now    #motivation\n",
       "...      ...                                                ...\n",
       "31958      0  ate @user isz that youuu?ðððððð...\n",
       "31959      0    to see nina turner on the airwaves trying to...\n",
       "31960      0  listening to sad songs on a monday morning otw...\n",
       "31961      1  @user #sikh #temple vandalised in in #calgary,...\n",
       "31962      0                   thank you @user for you follow  \n",
       "\n",
       "[31962 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31962 entries, 1 to 31962\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   31962 non-null  int64 \n",
      " 1   tweet   31962 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 749.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31967</th>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>thought factory: left-right polarisation! #tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>feeling like a mermaid ð #hairflip #neverre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49159</th>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17197 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "id                                                      \n",
       "31963  #studiolife #aislife #requires #passion #dedic...\n",
       "31964   @user #white #supremacists want everyone to s...\n",
       "31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "31966  is the hp and the cursed child book up for res...\n",
       "31967    3rd #bihday to my amazing, hilarious #nephew...\n",
       "...                                                  ...\n",
       "49155  thought factory: left-right polarisation! #tru...\n",
       "49156  feeling like a mermaid ð #hairflip #neverre...\n",
       "49157  #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49158  happy, at work conference: right mindset leads...\n",
       "49159  my   song \"so glad\" free download!  #shoegaze ...\n",
       "\n",
       "[17197 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17197 entries, 31963 to 49159\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   17197 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 268.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение датасета на train и val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 2), (6393, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df_train, \n",
    "                                    test_size=0.2, \n",
    "                                    random_state=10, \n",
    "                                    stratify=df_train['label'])\n",
    "\n",
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = set(punctuation)\n",
    "# Не будем очищать текст от апострофов, заменим их потом на пробелы,\n",
    "# т.к. встроенные в nltk английские стоп-слова и так потом отфильтруют лишнее\n",
    "puncts = puncts - {\"'\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Relict/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Relict/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = ''.join(char for char in txt if char not in puncts) # очистка от пунктуации\n",
    "    txt = txt.replace(\"'\", \" \")\n",
    "    txt = txt.lower().split()\n",
    "    txt = [word for word in txt if word.isalpha()] # очистка от символов и цифр\n",
    "    txt = [lemmatizer.lemmatize(word) for word in txt] # лемматизация\n",
    "    txt = [word for word in txt if word not in stopwords.words('english')] # очистка от стопслов\n",
    "    return ' '.join(txt)\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].apply(preprocess_text)\n",
    "df_val['tweet'] = df_val['tweet'].apply(preprocess_text)\n",
    "df_test['tweet'] = df_test['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28599</th>\n",
       "      <td>0</td>\n",
       "      <td>attending user user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27507</th>\n",
       "      <td>0</td>\n",
       "      <td>nimwit libtards embrace crookedhillary cronyga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16242</th>\n",
       "      <td>0</td>\n",
       "      <td>still sad pray today boanoite triste instagram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23285</th>\n",
       "      <td>0</td>\n",
       "      <td>user le month till york host one greatest annu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19694</th>\n",
       "      <td>0</td>\n",
       "      <td>user user user person change way living listen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "id                                                             \n",
       "28599      0                                attending user user\n",
       "27507      0  nimwit libtards embrace crookedhillary cronyga...\n",
       "16242      0  still sad pray today boanoite triste instagram...\n",
       "23285      0  user le month till york host one greatest annu...\n",
       "19694      0  user user user person change way living listen..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ''.join(df_train['tweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attending',\n",
       " 'user',\n",
       " 'usernimwit',\n",
       " 'libtards',\n",
       " 'embrace',\n",
       " 'crookedhillary',\n",
       " 'cronygarchy',\n",
       " 'braincell',\n",
       " 'anemicstill',\n",
       " 'sad']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(train_corpus)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 2000\n",
    "MAX_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'day', 'love', 'u', 'amp', 'like', 'life', 'happy', 'get', 'wa']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = FreqDist(tokens)\n",
    "tokens_top = [items[0] for items in dist.most_common(MAX_WORDS - 1)]\n",
    "\n",
    "tokens_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {word: count for count, word in dict(enumerate(tokens_top, 1)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переведём твиты в набор индексов, добавим паддинг\n",
    "\n",
    "def text_to_sequence(txt, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(txt)\n",
    "    for word in tokens:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "\n",
    "    padding = [0] * (maxlen-len(result))\n",
    "    return result[-maxlen:] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 20), (6393, 20))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_train['tweet'].values])\n",
    "X_val = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_val['tweet'].values])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Net(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, embedding_dim=512, out_dim=256, use_last=True, threshold=0.5, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n",
    "        self.gru = nn.GRU(embedding_dim, out_dim, batch_first=True) \n",
    "        self.linear = nn.Linear(out_dim, num_classes)\n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        self.use_last = use_last\n",
    "        \n",
    "    def forward(self, x):                          \n",
    "        x = self.embedding(x)\n",
    "        x = self.dp(x)\n",
    "        x, _ = self.gru(x)\n",
    "           \n",
    "        if self.use_last:\n",
    "            x = x[:,-1,:]\n",
    "        else:\n",
    "            x = torch.mean(x[:,:], dim=1)\n",
    "            \n",
    "        x = self.dp(x)\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = torch.IntTensor(x).to(device)\n",
    "        x = self.forward(x)\n",
    "        x = torch.squeeze((x > self.threshold).int())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GRU_Net                                  [1, 1]                    --\n",
       "├─Embedding: 1-1                         [1, 20, 512]              1,024,000\n",
       "├─Dropout: 1-2                           [1, 20, 512]              --\n",
       "├─GRU: 1-3                               [1, 20, 256]              591,360\n",
       "├─Dropout: 1-4                           [1, 256]                  --\n",
       "├─Linear: 1-5                            [1, 1]                    257\n",
       "==========================================================================================\n",
       "Total params: 1,615,617\n",
       "Trainable params: 1,615,617\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 12.85\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.12\n",
       "Params size (MB): 6.46\n",
       "Estimated Total Size (MB): 6.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(GRU_Net(), input_data=torch.IntTensor(X_train[np.newaxis, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.target = torch.from_numpy(target)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(10)\n",
    "\n",
    "train_dataset = DataWrapper(X_train, df_train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "val_dataset = DataWrapper(X_val, df_val['label'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(epochs, embedding_dim=512, hidden_size=256, lr=0.001, threshold=0.5):\n",
    "\n",
    "    torch.random.manual_seed(10)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    net = GRU_Net(vocab_size=MAX_WORDS, embedding_dim=embedding_dim, \n",
    "              out_dim=hidden_size, threshold=threshold).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    f1 = [0, 0.01]\n",
    "     \n",
    "    for epoch in range(epochs):\n",
    "        train_losses = np.array([])\n",
    "        test_losses = np.array([])\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        \n",
    "        if f1[epoch] < f1[epoch+1]:             \n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                net.train()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_losses = np.append(train_losses, loss.item())\n",
    "\n",
    "                net.eval()\n",
    "                outputs = torch.squeeze((net(inputs) > threshold).int())\n",
    "\n",
    "                tp += ((labels == 1) & (outputs == 1)).sum().item()\n",
    "                tn += ((labels == 0) & (outputs == 0)).sum().item()\n",
    "                fp += ((labels == 0) & (outputs == 1)).sum().item()\n",
    "                fn += ((labels == 1) & (outputs == 0)).sum().item()\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "            f1.append(f1_score)\n",
    "                     \n",
    "            print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
    "                  f'Loss: {train_losses.mean():.3f}. ' \\\n",
    "                  f'F1-score: {f1_score:.3f}', end='. ')\n",
    "            \n",
    "                        \n",
    "            tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, (inputs, labels) in enumerate(val_loader):\n",
    "\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "                    test_losses = np.append(test_losses, loss.item())\n",
    "\n",
    "                    tp += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                    tn += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "                    fp += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                    fn += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            print(f'Test loss: {test_losses.mean():.3f}. Test F1-score: {f1_score:.3f}.')\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print('Training is finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]. Loss: 0.299. F1-score: 0.174. Test loss: 0.180. Test F1-score: 0.444.\n",
      "Epoch [2/100]. Loss: 0.173. F1-score: 0.565. Test loss: 0.154. Test F1-score: 0.502.\n",
      "Epoch [3/100]. Loss: 0.151. F1-score: 0.633. Test loss: 0.144. Test F1-score: 0.585.\n",
      "Epoch [4/100]. Loss: 0.132. F1-score: 0.690. Test loss: 0.142. Test F1-score: 0.613.\n",
      "Epoch [5/100]. Loss: 0.122. F1-score: 0.723. Test loss: 0.138. Test F1-score: 0.600.\n",
      "Epoch [6/100]. Loss: 0.114. F1-score: 0.756. Test loss: 0.148. Test F1-score: 0.609.\n",
      "Epoch [7/100]. Loss: 0.105. F1-score: 0.777. Test loss: 0.135. Test F1-score: 0.625.\n",
      "Epoch [8/100]. Loss: 0.099. F1-score: 0.798. Test loss: 0.141. Test F1-score: 0.626.\n",
      "Epoch [9/100]. Loss: 0.093. F1-score: 0.821. Test loss: 0.154. Test F1-score: 0.640.\n",
      "Epoch [10/100]. Loss: 0.087. F1-score: 0.836. Test loss: 0.143. Test F1-score: 0.634.\n",
      "Epoch [11/100]. Loss: 0.077. F1-score: 0.865. Test loss: 0.163. Test F1-score: 0.629.\n",
      "Epoch [12/100]. Loss: 0.074. F1-score: 0.872. Test loss: 0.160. Test F1-score: 0.639.\n",
      "Epoch [13/100]. Loss: 0.071. F1-score: 0.883. Test loss: 0.160. Test F1-score: 0.640.\n",
      "Epoch [14/100]. Loss: 0.065. F1-score: 0.894. Test loss: 0.171. Test F1-score: 0.641.\n",
      "Epoch [15/100]. Loss: 0.062. F1-score: 0.912. Test loss: 0.171. Test F1-score: 0.640.\n",
      "Epoch [16/100]. Loss: 0.060. F1-score: 0.919. Test loss: 0.185. Test F1-score: 0.641.\n",
      "Epoch [17/100]. Loss: 0.055. F1-score: 0.936. Test loss: 0.168. Test F1-score: 0.631.\n",
      "Epoch [18/100]. Loss: 0.050. F1-score: 0.944. Test loss: 0.216. Test F1-score: 0.639.\n",
      "Epoch [19/100]. Loss: 0.051. F1-score: 0.947. Test loss: 0.187. Test F1-score: 0.637.\n",
      "Epoch [20/100]. Loss: 0.047. F1-score: 0.958. Test loss: 0.200. Test F1-score: 0.625.\n",
      "Epoch [21/100]. Loss: 0.044. F1-score: 0.956. Test loss: 0.211. Test F1-score: 0.630.\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: наилучшие показатели по тестовым данным получены на 9 эпохе  f1-score составил 0,64 (параметры нейронной сети приняты аналогично урока №6). f1-score по уроку №7 показал большую точность чем было по уроку №6. На 21 эпохе с началом переобучения нейронной сети цикл обучения был остановлен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lecture07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
