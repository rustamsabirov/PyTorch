{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aP3VqmLJ8jK"
   },
   "source": [
    "# Dataset, Dataloader, BatchNorm, Dropout, Оптимизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1kh-e0oXz7V"
   },
   "source": [
    "# Домашнее задание\n",
    "\n",
    "Будем практиковаться на датасете недвижимости (sklearn.datasets.fetch_california_housing или  https://www.kaggle.com/c/avito-demand-prediction)\n",
    "\n",
    "Ваша задача:\n",
    "1. Создать Dataset для загрузки данных\n",
    "2. Обернуть его в Dataloader\n",
    "3. Написать архитектуру сети, которая предсказывает стоимость недвижимости (или которая предсказывает число показов соответственно) на основании числовых данных. Можно сделать генерацию новых признаков. Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
    "4. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели\n",
    "\n",
    "train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "housedata=fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n",
       " 'frame': None,\n",
       " 'target_names': ['MedHouseVal'],\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block group\\n        - HouseAge      median house age in block group\\n        - AveRooms      average number of rooms per household\\n        - AveBedrms     average number of bedrooms per household\\n        - Population    block group population\\n        - AveOccup      average number of household members\\n        - Latitude      block group latitude\\n        - Longitude     block group longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nAn household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surpinsingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим на тестовые и тренеровочные данные\n",
    "X_train,X_test,y_train,y_test=train_test_split(housedata.data,housedata.target,test_size=0.25,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализуем данные и подготовим их для дальнейшего использования в нашем dstaset\n",
    "scale = StandardScaler()\n",
    "X_train_s = scale.fit_transform(X_train)\n",
    "X_test_s = scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(X_train_s.astype(np.float32)).to(DEVICE)\n",
    "train_y = torch.from_numpy(y_train.astype(np.float32)).to(DEVICE)\n",
    "\n",
    "test_x = torch.from_numpy(X_test_s.astype(np.float32)).to(DEVICE)\n",
    "test_y = torch.from_numpy(y_test.astype(np.float32)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RangeDataset(Dataset):\n",
    "    def __init__(self, *init_datasets):\n",
    "        assert all(init_datasets[0].size(0) == init_dataset.size(0) for init_dataset in init_datasets), \"Несоотвутствует размерность среди dataset\"\n",
    "        self._base_datasets = init_datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._base_datasets[0].size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return tuple(base_dataset[idx] for base_dataset in self._base_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RangeDataset(train_x, train_y)\n",
    "test_dataset = RangeDataset(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(My_Net, self).__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=64, bias=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU())\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=64, out_features=32, bias=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU())\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=16),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU())    \n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(in_features=16, out_features=1, bias=True),\n",
    "#             nn.BatchNorm1d(1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.block_1(inp)\n",
    "        out = self.block_2(out)\n",
    "        out = self.block_3(out)\n",
    "        out = self.predict(out)\n",
    "        return out[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, test_loader, net, optimizer):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    best_acc = {'train': None, 'test': None}\n",
    "    net.train()\n",
    "    for epoch in range(EPOCHES):\n",
    "        running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "\n",
    "        if i % 150 == 0 or (i + 1) == len(train_loader):    # печатаем каждые 300 mini-batches\n",
    "            net.eval()\n",
    "\n",
    "            test_loss, test_running_total, test_loss  = 0.0, 0.0, 0.0\n",
    "            for y, (out_test, lbl_test) in enumerate(test_loader):\n",
    "                test_outputs = net(out_test)\n",
    "                test_loss += loss_fn(test_outputs, lbl_test)\n",
    "                test_running_total += len(lbl_test)\n",
    "\n",
    "            res_loss_train = running_loss / running_items\n",
    "            res_loss_test = test_loss / test_running_total\n",
    "\n",
    "            if best_acc['train'] is None or res_loss_train < best_acc['train']:\n",
    "                best_acc['train'] = res_loss_train\n",
    "\n",
    "            if best_acc['test'] is None or res_loss_test < best_acc['test']:\n",
    "                best_acc['test'] = res_loss_train\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{EPOCHES}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {res_loss_train:.3f}. '\\\n",
    "                  f'Test acc: {res_loss_test:.3f}.')\n",
    "\n",
    "            running_loss, running_items = 0.0, 0.0\n",
    "            net.train()\n",
    "    print(f\"Best acc train: {best_acc['train']:.3f}. Best acc test: {best_acc['test']:.3f}\")\n",
    "    print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### РАБОТА НЕЙРОННОЙ СЕТИ С РАЗНЫМИ ОПТИМИЗАТОРАМИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = My_Net().to(DEVICE)\n",
    "optimizer = Adam(net.parameters(), lr=0.001,betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Step [483/483]. Loss: 0.152. Test acc: 0.117.\n",
      "Epoch [2/10]. Step [483/483]. Loss: 0.077. Test acc: 0.116.\n",
      "Epoch [3/10]. Step [483/483]. Loss: 0.081. Test acc: 0.116.\n",
      "Epoch [4/10]. Step [483/483]. Loss: 0.089. Test acc: 0.116.\n",
      "Epoch [5/10]. Step [483/483]. Loss: 0.099. Test acc: 0.116.\n",
      "Epoch [6/10]. Step [483/483]. Loss: 0.088. Test acc: 0.117.\n",
      "Epoch [7/10]. Step [483/483]. Loss: 0.166. Test acc: 0.116.\n",
      "Epoch [8/10]. Step [483/483]. Loss: 0.155. Test acc: 0.116.\n",
      "Epoch [9/10]. Step [483/483]. Loss: 0.116. Test acc: 0.116.\n",
      "Epoch [10/10]. Step [483/483]. Loss: 0.140. Test acc: 0.116.\n",
      "Best acc train: 0.077. Best acc test: 0.077\n",
      "Training is finished!\n",
      "Wall time: 6.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loop(train_loader, test_loader, net, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = My_Net().to(DEVICE)\n",
    "optimizer = SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Step [483/483]. Loss: 0.075. Test acc: 0.116.\n",
      "Epoch [2/10]. Step [483/483]. Loss: 0.085. Test acc: 0.117.\n",
      "Epoch [3/10]. Step [483/483]. Loss: 0.137. Test acc: 0.116.\n",
      "Epoch [4/10]. Step [483/483]. Loss: 0.119. Test acc: 0.116.\n",
      "Epoch [5/10]. Step [483/483]. Loss: 0.085. Test acc: 0.116.\n",
      "Epoch [6/10]. Step [483/483]. Loss: 0.084. Test acc: 0.116.\n",
      "Epoch [7/10]. Step [483/483]. Loss: 0.101. Test acc: 0.115.\n",
      "Epoch [8/10]. Step [483/483]. Loss: 0.152. Test acc: 0.116.\n",
      "Epoch [9/10]. Step [483/483]. Loss: 0.102. Test acc: 0.116.\n",
      "Epoch [10/10]. Step [483/483]. Loss: 0.140. Test acc: 0.116.\n",
      "Best acc train: 0.075. Best acc test: 0.075\n",
      "Training is finished!\n",
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loop(train_loader, test_loader, net, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = My_Net().to(DEVICE)\n",
    "optimizer = RMSprop(net.parameters(), lr=0.001, alpha=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Step [483/483]. Loss: 0.061. Test acc: 0.102.\n",
      "Epoch [2/10]. Step [483/483]. Loss: 0.130. Test acc: 0.102.\n",
      "Epoch [3/10]. Step [483/483]. Loss: 0.081. Test acc: 0.102.\n",
      "Epoch [4/10]. Step [483/483]. Loss: 0.095. Test acc: 0.100.\n",
      "Epoch [5/10]. Step [483/483]. Loss: 0.121. Test acc: 0.102.\n",
      "Epoch [6/10]. Step [483/483]. Loss: 0.079. Test acc: 0.099.\n",
      "Epoch [7/10]. Step [483/483]. Loss: 0.075. Test acc: 0.101.\n",
      "Epoch [8/10]. Step [483/483]. Loss: 0.102. Test acc: 0.103.\n",
      "Epoch [9/10]. Step [483/483]. Loss: 0.092. Test acc: 0.101.\n",
      "Epoch [10/10]. Step [483/483]. Loss: 0.063. Test acc: 0.099.\n",
      "Best acc train: 0.061. Best acc test: 0.061\n",
      "Training is finished!\n",
      "Wall time: 6.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loop(train_loader, test_loader, net, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: Лучший результат (по точности) показала нейронная сеть с оптимизизатором Adam, немного хуже нейронная сеть с оптимизизатором SGD, самый худший результат нейронная сеть с оптимизизатором RMSprop. Сети с RMSprop понадобилось больше попыток прогона для схождения результатов тренировочного и тестового датасета. Для задач подобного рода лучше приспособлены оптимизаторы ADAM и SGD   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1LJj_5vSuGnFQ1mArvQfPJ4K7Ax-vAmQg",
     "timestamp": 1656423881563
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
